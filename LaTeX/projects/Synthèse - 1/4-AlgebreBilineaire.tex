\chapter*{\chapterstyle{V --- Espaces Quadratiques}}
\addcontentsline{toc}{section}{Espaces Quadratiques}
On se donne un espace vectoriel \(E\), on dira que c'est un \textbf{espace quadratique} si et seulement si on peut définir une \textbf{forme bilinéaire symétrique} sur cet espace.\<

Alors on pourra alors définir une \textbf{forme quadratique} sur cet espace qui est une application \(q : E \rightarrow \K\) telle que pour une certaine forme bilinéaire symétrique \(f\), on ait:
\[
   q(x) = f(x, x)   
\]
On appelera alors le membre de droite \textbf{forme polaire} de la forme quadratique \(q\). On remarquera par la suite que moralement ces deux applications s'interpètent de la manière suivante:
\begin{itemize}
   \item Une forme bilinéaire symétrique mesure des "angles" dans l'espace.
   \item Une forme quadratique mesure des "longeurs" dans l'espace.
\end{itemize}
Mais dans le cas général d'un espace quadratique et sans plus d'hypothèses, ces "angles" et "longueurs" ne correspondent pas vraiment aux concepts géométriques que l'on connaît. Tout l'algèbre bilinéaire développée dans ce chapitre peut se résumer à étudier des formes primitives qui par la suite permettront d'axiomatiser la notion de \textbf{produit scalaire} qui incarnera les propriétés géométriques recherchées. 

\subsection*{\subsecstyle{Formules de polarisation{:}}}
On considère une forme quadratique \(q\) quelconque et on souhaiterait reconstituer la forme polaire de \(q\), alors on peut montrer qu'elle vérifie les \textbf{identités de polarisation} ci-dessous:
\[
   \begin{cases}
      f(x, y) = \frac{1}{4}(q(x + y) - q(x - y))\\
      f(x, y) = \frac{1}{2}(q(x + y) - q(x) - q(y))\\
   \end{cases}   
\]
C'est identités se retrouvent aisément en considérant la forme quadratique triviale sur \(\R\) associée à \(f(x, y) = xy\) qui est donc \(q(x) = x^2\).
\subsection*{\subsecstyle{Expression matricielle{:}}}
En dimension finie, on peut représenter les vecteurs \(x, y \in E\) dans une base \(\mathscr{B} = (e_i)\), on peut alors développer par bilinéarité et symétrie pour obtenir:
\customBox{width=5.5cm}{
   \[
      f(x, y) = \sum_{1 \leq i, j \leq n}x_iy_j f(e_i, e_j)   
   \]
}
En particulier on peut alors remarquer que la forme bilinéaire est parfaitement déterminée par la donnée des \(n^2\) images des paires des vecteurs de la base. En particulier, en notant \(X, Y\) les vecteurs colonnes des coordonées de \(x, y\) dans la base, alors on peut montrer qu'il existe une matrice \(A\) telle que:
\customBox{width = 4cm}{
   \(
      [f(x,y)]^\mathscr{B} = X^\top A Y
   \)
}
Et cette matrice est alors de la forme suivante:
\[
   (a_{i, j}) = f(e_i, e_j)
\]
En particulier cette matrice est symétrique et représente parfaitement la forme bilinéaire symétrique et la forme quadratique\footnote[1]{Il suffit de calculer les coordonées de \(f(x, x)\) pour le voir, on trouve alors qu'elle sont égales à \(X^\top A X\)}.
\subsection*{\subsecstyle{Règle du dédoublement{:}}}
Supposont que l'on connaisse une expression analytique de \(q(x)\) dans une base \((e_i, \ldots, e_n)\), on a alors:
\[
   q(x) = \sum_{1 \leq i, j \leq n} x_ix_j f(x_i, x_j) = \sum_{1 \leq i, j \leq n} x_ix_j b_{i, j}
\]
Et donc par symétrique du produit, on peut alors retrouver la matrice \(A = (a_{i,j})\) de \(q\) par la règle dite du \textbf{dédoublement des termes}, ie:
\[
   \begin{cases}
      a_{i,j} &= b_{i, j} \text{ si } i = j\\
      a_{i,j} &= \frac{1}{2} b_{i, j} \text{ si } i \neq j\\
   \end{cases}   
\]
\uline{Exemple:} On considère la forme quadratique suivante sur \(\R^2\):
\[
   q(x, y) = 3x^2 + 5y^2 + 8xy
\]
Alors la règle du dédoublement des termes nous donne que la matrice de \(q\) dans la base canonique est:
\[
   M = \begin{pmatrix}
      3 & 4 \\
      4 & 5
   \end{pmatrix}   
\]
\subsection*{\subsecstyle{Orthogonalité{:}}}
On peut alors définir une notion d'orthogonalité\footnote[1]{\textbf{Attention:} Pour l'instant ceci n'est \textbf{pas} une notion géométrique, mais purement algébrique.} pour la forme bilinéaire \(f\), qu'on appelera \(f\)-orthogonalité, et on dira alors: 
\begin{center}
   \textbf{Deux vecteurs sont \(f\)-orthogonaux si et seulement si \(f(x, y) = 0\).}
\end{center}
On notera alors \(x \perp y\). On peut alors définir le concept de famille \(f\)-orthogonale, ainsi que le concept de base \(f\)-orthogonale, comme une famille telle que tout les vecteurs soient deux à deux orthogonaux.\<

\textbf{Attention:} Dans le cas général, une famille orthogonale n'est pas forcément libre ! Il suffit de considérer \(q(x, y) = x^2\) et la famille \((0, 1), (0, 1)\). C'est une nouvelle conséquence du fait que cette notion d'orthogonalité n'est \textbf{pas géométrique}.
\subsection*{\subsecstyle{Orthogonal d'une partie{:}}}
Soit \(A\) une partie de \(E\), alors on peut définir l'orthogonal de \(A\) comme \textbf{le sous-espace vectoriel} défini par:
\customBox{width=6cm}{
   \(A^\perp := \bigl\{ u \in E \; ; \; \forall v \in A \, , \, u \perp v  \bigl\}\) 
}
En dimension finie, on a alors une base \((e_1, \ldots, e_n)\) de \(F\) et on a la caractérisation suivante:
\customBox{width=7cm}{
   \(F^\perp := \bigl\{ u \in E \; ; \; \forall i \in \inticc{1}{n} \, , \, u \perp e_i  \bigl\}\)
}
On peut alors montrer les propriétés suivantes pour l'application qui a une partie associe son orthogonal:
\begin{itemize}
   \item La décroissance du passage à l'orthogonal.
   \item L'inclusion de la partie dans son double orthogonal.
\end{itemize}
\subsection*{\subsecstyle{Noyau{:}}}
Pour une forme bilinéaire quelconque, il est possible que l'ensemble \(E^\perp\) soit non-trivial, on appelle alors cet ensemble \textbf{noyau} de la forme bilinéaire, cette dénomination\footnote[2]{\textbf{Attention:} Ici on montre que le noyau de la forme bilinéaire est défini par le noyau de l'application linéaire associée à sa matrice, c'est non-trivial.} venant de la raison ci-dessous, pour \(M\) la matrice de la forme bilinéaire:
\[
   E^\perp = \ker(M)
\]
\pagebreak

Avec ces définitions il est alors possible de montrer un analogue à la \textbf{formule du rang}:
\[
   \dim(\Ker{q}) + \dim(\Im{q}) = \dim(E)
\]
Si ce noyau est trivial, on dira alors que la forme bilinéaire est \textbf{non-dégénérée} et en \textbf{dimension finie} on a alors les égalités suivantes:
\[
   \begin{cases}
      \dim(F^\perp) + \dim(F) = \dim(E)\\
      F = F^{\perp^\perp}
   \end{cases}
\]
Attention, ici il est important de noter que même pour une forme non-dégénérée, on n'a \textbf{pas la supplémentarité à priori}.
\subsection*{\subsecstyle{Cône isotrope{:}}}
Pour une forme bilinéaire quelconque, il est aussi possible que l'ensemble \(\{ x \in E ; f(x, x) = q(x) = 0\}\) soit non-trivial, ici il s'agit des vecteurs dont la "longueur" est nulle, on appelera alors ces vecteurs \textbf{vecteurs isotropes}. On a alors directement l'inclusion suivant:
\begin{center}
   \textbf{Le noyau est inclu dans le cône isotrope.}
\end{center}
Si le cône isotrope est trivial, alors on dira que la forme est \textbf{définie}.
\subsection*{\subsecstyle{Cas particulier des formes réelles{:}}}
Soit \(f\) une forme bilinéaire symétrique réelle, alors on classifie ces formes par:
\begin{itemize}
   \item Si \(\forall x \in E, f(x, x) \geq 0\), on dira que la forme est \textbf{positive}.
   \item Si \(\forall x \in E, f(x, x) \leq 0\), on dira que la forme est \textbf{négative}.
\end{itemize}
\subsection*{\subsecstyle{Réduction{:}}}
On cherche maintenant à trouver une base \(\mathscr{B}\) de \(E\) telle que l'expression de \(f(x, y)\) soit plus simple, en particulier on essaye de trouver une base telle que sa matrice soit \textbf{diagonale}, et on peut alors facilement montrer que c'est équivalent à \textbf{chercher une base \(f\)-orthogonale}. On a alors dans une telle base:
\[
   q(x) = \sum_{i\in I}q(e_i)x_i^2 = \sum_{i \in I}q(e_i)e_i^*(x)^2
\]
On peut montrer le théorème suivant:
\begin{center}
   \textbf{Il existe des formes linéaires indépendantes telles que \(q(x) = q(e_1)l_1(x)^2+\ldots+q(e_n)l_n(x)^2\)}
\end{center}
En particulier, ce théorème se démontre de manière constructive via un algorithme qui nous permettra de réduire tout forme bilinéaire en somme de carrés de formes linéaires indépendantes, c'est la \textbf{réduction de Gauss}, une fois les formes linéaires explicitées, il faut alors compléter la famille de formes linéaires en une base du dual, et la base orthogonale recherchée sera \textbf{la préduale de cette base}.
\subsection*{\subsecstyle{Algorithme de Gauss{:}}}
On se donne une forme quadratique suivante à décomposer:
\[
   q(x) = \sum_{i, j}c_{i, j}x_ix_j
\]
L'algorithme est récursif et comporte deux cas:
\begin{itemize}
   \item La forme quadratique comporte un terme carré de la forme \(c_{i,i}x_i^2\)
   \item La forme quadratique ne comporte pas de termes carrés de la forme \(c_{i,i}x_i^2\)
\end{itemize}
\pagebreak

Traitons ces deux cas séparément:
\begin{itemize}
   \item Dans le premier cas, on regroupe tout les termes qui comportent le terme carré et on applique la formule \(A^2 + BA = (A + \frac{B}{2})^2 - \frac{B}{2}^2\) ce qui fait apparaître un carré de forme linéaire.
   \item Dans le second cas on regroupe tout les termes qui contiennent deux variables choisies et la formule \(AB = \frac{1}{4}((A + B)^2 - (A - B)^2)\) ce qui fait apparaître des carrés de formes linéaires.
\end{itemize}
\uline{Exemple:} On pose la forme quadratique suivante:
\[
   q(x, y, z, t) = x^2 + 2xy + y^2 - 4yz
\]
On commence par isoler les termes en \(x\) et appliquer la formule du premier cas, on a donc notre premier carré de forme linéaire:
\[
   q(x, y, z, t) = (x^2 + 2xy) +y^2 - 4yz = (x + y)^2 -y^2 + y^2 - 4yz = l_1(x, y, z, t)^2 - 4yz
\]
Maintenant on réapplique l'algorithme à la forme quadratique restante \(\tilde{q}(x, y, z) = -4yz\), on applique la formule du second cas et on a:
\[
   \tilde{q}(x, y, z, t) = -4yz = -((y + z)^2 - (y - z)^2) = -l_2(x, y, z, t)^2 + l_3(x, y, z, t)^2
\]
Finalement on trouve:
\[
   q(x, y, z, t) = l_1(x, y, z, t)^2 - l_2(x, y, z, t)^2 + l_3(x, y, z, t)^2 = (x + y)^2 - (y + z)^2 + (y - z)^2
\]
On a alors trouvé les formes linéaires recherchées \(l_1, l_2, l_3\) et pour trouver une base \(f\)-orthogonale de \(\R^4\), il reste encore la dernière étape:
\begin{center}
   \textbf{On complète \((l_1, l_2, l_3)\) en une base du dual de \(E\) et la base recherchée est alors la préduale de celle-ci.}
\end{center}
\subsection*{\subsecstyle{Signature{:}}}
On définir alors la \textbf{signature d'une forme bilinéaire} comme étant le couple d'entiers \((p, q)\) avec:
\begin{itemize}
   \item L'entier \(p\) est \textbf{le nombre de valeurs propres strictement positives}.
   \item L'entier \(q\) est \textbf{le nombre de valeurs propres strictement négatives}.
\end{itemize}
On peut alors montrer \textbf{la loi d'inertie de Sylvester} et si on a la forme réduite:
\[
   q(x) = \alpha_1l_1(x)^2 + \ldots + \alpha_nl_n(x)^2
\]
Alors on cette loi nous donne que \(p\) est exactement le nombre de coefficients \(\alpha_i\) strictement positifs, et \(q\) le nombre de coefficients négatifs.
\chapter*{\chapterstyle{V --- Espaces Préhilbertiens Réels}}
\addcontentsline{toc}{section}{Espaces Préhilbertiens Réels}
On appelle \textbf{espace préhilbertien réel} un \(\R\)-espace vectoriel muni d'une forme \textbf{bilinéaire symétrique définie et positive}, c'est à dire une forme qui vérifie:

\customBox{width=7cm}{
   \begin{align*}
      &\textbf{Symétrie} &&f(u, v) = f(v, u) \\
      &\textbf{Définie} &&f(u, u) = 0 \implies u = 0 \\
      &\textbf{Positivité} &&f(u, u) \geq 0
   \end{align*}
}   
On dira alors qu'une telle forme est un \textbf{produit scalaire} sur \(E\) et on notera:
\[
   f(x, y) = \dotproduct{x}{y}
\]
Cet espace, qui est un cas particulier d'espace quadratique, est celui où se réalisera la signification \textbf{géométrique} des formes bilinéaires et quadratiques, gràce aux nouvelles contraintes sur ces formes.
\subsection*{\subsecstyle{Exemples {:}}}
\begin{itemize}
   \item On définit sur \(\R^n\) le produit scalaire défini par:
   \[
      \dotproduct{u}{v} = \sum_{k=1}^{n} u_kv_k    
   \]
   \item On définit sur \(\mathcal{C}^1(\icc{0}{1}, \R)\) le produit scalaire défini par:
   \[
      \dotproduct{f}{g} = \int_{0}^{1} f(t)g(t) d t    
   \]
   \item On définit sur \(\R_n[X]\) le produit scalaire défini pour \((x_n)\) \(n + 1\) points fixés par:
   \[
      \dotproduct{P}{Q} = \sum_{k=0}^{n}P(x_k)Q(x_k)
   \]   
   \item On définit sur \(\mathcal{M}_n(\R)\) le produit scalaire défini par:
   \[
      \dotproduct{A}{B} = \text{tr}(A^\top B)
   \]
\end{itemize}
\subsection*{\subsecstyle{Norme {:}}}
A partir de la définition d'un tel espace, on alors montrer que \(\dotproduct{\cdot}{\cdot}\) \textbf{induit une norme} sur \(E\) donnée par la \textbf{forme quadratique associée}:
\customBox{width=5cm}{
   \(
      \forall u \in E \; ; \; \vectNorm{u}^2 = \dotproduct{u}{u}
   \)
}
Cela fait donc de \(E\) un espace vectoriel normé.
\subsection*{\subsecstyle{Angle{:}}}
A partir de ces définitions, on peut alors définir \textbf{l'angle non-orienté} \(\theta \in \icc{0}{\pi}\) entre deux vecteurs \(u, v\) par:
\customBox{width=4.5cm}{
   \[
      \theta := \cos^{-1}{\left(\frac{\dotproduct{u}{v}}{\vectNorm{u}\vectNorm{v}}\right)}   
   \]
}
Ce qui nous permet de caractériser \textbf{l'orthogonalité} du produit scalaire comme un orthogonal \textbf{géométrique}, en effet \(\theta \equiv \frac{\pi}{2}\) dans cette définition ssi \(\dotproduct{u}{v} = 0\)
\begin{center}
   \textit{L'interprétation de cet "angle" ou de "l'orthogonalité" entre deux vecteurs diffère selon le contexte, elle peut alors signifier une corrélation en probabilité, ou un réel angle géométrique dans \(\R^n\) par exemple.}
\end{center}
\subsection*{\subsecstyle{Inégalité de Cauchy-Schwarz{:}}}
Dans tout espace préhilbertien réel, pour tout \(u, v \in E\) on a l'inégalité\footnote[1]{Trés puissante et permet d'obtenir des majorations dans des cas trés variés, la preuve parte de l'étude du polynôme \(P(t) = \vectNorm{x + ty}^2\)} suivante:
\customBox{width=4cm}{
   \(
      |\dotproduct{u}{v}| \leq \vectNorm{u}\,\vectNorm{v}
   \)
}
Avec cas d'égalité quand \(u, v\) sont \textbf{liés}.
\subsection*{\subsecstyle{Formules géométriques{:}}}
Dans tout espace préhilbertien réel, pour tout \(u, v \in E\) on a les identités suivantes:
\begin{itemize}
   \item \textbf{Identité du parallélogramme :} \(\vectNorm{x + y}^2 + \vectNorm{x - y}^2 = 2\vectNorm{x}^2 + 2\vectNorm{y}^2\).

   \item \textbf{Théorème de Pythagore :} \(x \perp y \Longleftrightarrow \vectNorm{x + y}^2 = \vectNorm{x}^2 + \vectNorm{y}^2\).
\end{itemize}
La première caractérise les espaces normés telles que leur norme soit issue d'un produit scalaire.
\subsection*{\subsecstyle{Orthogonalité{:}}}
Dans le cadre des espaces préhilbertiens, l'orthogonal obtient alors une partie des propriétés géométriques intuitives qu'on lui connait, en particulier:
\begin{itemize}
   \item Une famille orthogonale de vecteurs non-nuls est toujours libre.
   \item Une partie et son orthogonal sont toujours en somme directe.
\end{itemize}
Attention dans un espace préhilbertien quelconque, ils ne sont pas toujours supplémentaires, on verra que c'est le cas en dimension finie !
\subsection*{\subsecstyle{Théorème de représentation{:}}}
On se donne un élément \(\phi\) du dual d'un espace préhilbertien alors, dans ce cadre, et même de manière générale dans celui des espaces de Hilbert, on peut montrer \textbf{le théorème de représentation de Riesz} qui caractérise une forme linéaire \(\phi\) par le produit scalaire:
\[
   \exists w \in E \; , \; \forall x \in E \; ; \; \phi(x) = \dotproduct{w}{x}   
\]
Simplement, cela signifie que toute forme linéaire est \textbf{exactement représentée} par le produit scalaire pour un certain vecteur \(w\), en particulier, on a donc une bijection entre \(E\) et son dual.\<

\uline{Exemple}: On prends la forme linéaire \(\phi(x, y ,z) = 5x+4y+3z\) et on munit \(\R^3\) de son produit scalaire canonique, alors pour tout \(u\) on a directement que:
\[
   \phi(u) = \dotproduct{(5, 4, 3)}{u}
\]
\subsection*{\subsecstyle{Transposition{:}}}
On se donne \(f \in \mathcal{L}(E, F)\) représentée par une matrice \(M \in \mathcal{M}_{n, p}(\R)\), alors on définit \textbf{l'application transposée} de \(f\) par:
\[
   \begin{aligned}
      f^\top : F^* &\longrightarrow E^*\\
      \phi &\longmapsto \phi \circ f
   \end{aligned}
\]
On vérifie alors que cette application est bien définie est on a alors la propriété suivante:
\begin{center}
   \textbf{L'application transposée est représentée dans les bases correspondates par la matrice transposée.}
\end{center}
Ce qui donne finalement une interprétation fonctionnelle de la matrice transposée. A FINIR, LA TRANSPOSEE EST EXACTEMENT LADJOINT, LIEN AVEC TOUT LE RESTE A FAIRE, UNIQUE APPLICATION QUI VERFIE <f(x), y> = <x, tf(y)>, DUALITE.
\chapter*{\chapterstyle{V --- Espaces Euclidiens}}
\addcontentsline{toc}{section}{Espaces Euclidiens}
On appelle \textbf{espace euclidien} tout espace préhilbertien réel \textbf{de dimension finie}. Dans toute la suite on prendra \((e_1, \ldots, e_n)\) une base de \(E\).

En particulier, dans une base \textbf{orthonormée}, on a \(\dotproduct{x}{e_i} = x_i\) et donc:
\[
   x = \sum_{k=1}^{n} \dotproduct{x}{e_i}e_i 
\]
\subsection*{\subsecstyle{Orthogonalité{:}}}
En dimension finie, on a finalement l'ensemble des propriétés géométriques de l'orthogonalité qui deviennent vraies, en effet on a::
\customBox{width = 4cm}{
   \(
     F \oplus F^\perp = E 
   \)
}
\begin{center}
   \textit{Tout sous-espace admet un unique supplémentaire orthogonal.}
\end{center}
On peut alors en déduire qu'en dimension finie on a:
\[
   (F^\perp)^\perp = F   
\]
\subsection*{\subsecstyle{Projection orthogonale{:}}}
L'existence d'une unique décomposition nous permet alors de définir \textbf{la projection} sur \(F\) de direction \(F^\perp\) par:
\[
   \text{proj}_F : x = x_F + x_{F^\perp} \mapsto x_F   
\]
En particulier, on en déduit par l'unicité de la décomposition que \(\text{proj}_F(x)\) est \textbf{l'unique vecteur} de \(F\) qui vérifie:
\customBox{width=4cm}{
   \(x - \text{proj}_F(x) \in F^\perp\)
}
Le projeté orthogonal a une signification géométrique importante, en effet on a:
\customBox{width=7cm}{
   \(\vectNorm{x - \text{proj}_F(x)} = \min_{y\in F}(\vectNorm{x - y})\)
}
\begin{center}
   \textit{C'est le vecteur "le plus proche" de \(F\) au sens du produit scalaire utilisé.}
\end{center}
\pagebreak
\subsection*{\subsecstyle{Calcul de projeté orthogonal{:}}}
On considère un sous-espace \(F\) de bases \((e_1, \ldots, e_p)\) et \(x \in E\), on sait d'aprés les propriétés précédentes que \(\text{proj}_F(x)\) est l'unique vecteur de \(F\) tel que \(x - \text{proj}_F(x) \in F^\perp\), ce qui est équivalent à dire que:
\begin{align*}
   &\forall j \in \inticc{1}{p} \; ; \; \dotproduct{x - \text{proj}_F(x)}{e_j} = 0 \Longleftrightarrow \\
   &\forall j \in \inticc{1}{p} \; ; \; \dotproduct{\text{proj}_F(x)}{e_j} = \dotproduct{x}{e_j}
\end{align*}
On raisonne alors par coefficient indeterminés avec l'écriture de \(\text{proj}_F(x) = \sum_{i=1}^{n} \alpha_i e_i\) dans la base de \(F\) pour obtenir l'expression suivante:
\[
   \forall j \in \inticc{1}{p} \; ; \; \sum_{i=1}^{n}\alpha_k \dotproduct{e_i}{e_j} = \dotproduct{x}{e_i}
\] 
Enfin on obtient alors \textbf{le système des équations normales}:
\[
   \begin{cases}
      \alpha_1 \dotproduct{e_1}{e_1} + \alpha_2 \dotproduct{e_2}{e_1} + \ldots + \alpha_p \dotproduct{e_p}{e_1} = \dotproduct{x}{e_1}\\
      \vdots \\
      \alpha_1 \dotproduct{e_1}{e_p} + \alpha_2 \dotproduct{e_2}{e_p} + \ldots + \alpha_p \dotproduct{e_p}{e_p} = \dotproduct{x}{e_p}
   \end{cases} 
\]
Ou de manière équivalente pour \(M\) la matrice du produit scalaire dans la base de \((e_1, \ldots, e_p)\):
\[
   MY = 
   \begin{bmatrix}
      \dotproduct{x}{e_1}\\
      \vdots \\
      \dotproduct{x}{e_p}
   \end{bmatrix}
\]
Dans le cas d'une base \textbf{orthogonale}, le système ci-dessus est beaucoup plus simple, en effet presque tout les produits scalaires sont nuls, et on obtient un système \textbf{diagonal} et donc dans ce cas précis, le projeté orthogonal s'obtient simplement par la formule:
\customBox{width=5cm}{
   \[
      \text{proj}_F(x) = \sum_{k = 1}^{p} \frac{\dotproduct{x}{e_i}}{\dotproduct{e_i}{e_i}}e_i     
   \]
}
Finalement, une remarque importante permet de comprendre la projection sur un sous espace doté d'une base orthogonale, en effet:
\begin{center}
   \textit{Projeter un vecteur sur un sous-espace revient à ajouter les projetés de ce vecteur sur les vecteurs de la base du sous-espace.}
\end{center}
\underline{Exemple:} Le projeté de \(x = (1, 2, 3)\) sur le plan \(\text{Vect}((1, 0, 0), (0, 1, 0))\) est donné par \(\text{proj}_{(1, 0, 0)}(x) + \text{proj}_{(0, 1, 0)}(x)\)
\subsection*{\subsecstyle{Procédé de Gramm-Schmidt{:}}}
Soit \((e_1, \ldots, e_n)\) une base de \(E\), on cherche alors à élaborer un procédé permettant \textbf{d'orthogonaliser cette base} en une base \((\epsilon_1, \ldots, \epsilon_n)\), on pose \(\epsilon_1 = e_1\) et \(H_i = \text{Vect}(e_1, \ldots, e_i)\) et on définit par récurrence:
\customBox{width=12cm}{
   \[
      \epsilon_i = e_i - \text{proj}_{H_{i-1}}(e_i) = e_i - \left(\sum_{k=0}^{i - 1}\text{proj}_{\epsilon_k}(e_k)\right) = e_i - \left(\sum_{k=0}^{i - 1}\frac{\dotproduct{e_i}{\epsilon_i}}{\dotproduct{\epsilon_i}{\epsilon_i}}\epsilon_i \right)
   \]
}

\begin{center}
   \textit{Moralement, on "redresse" chaque vecteur de la base initiale en lui retirant son défaut d'orthogonalité représenté par sa projection sur le sous-espace précédent.}
\end{center}

\chapter*{\chapterstyle{V --- Espaces Hemitiens}}
\addcontentsline{toc}{section}{Espaces Hemitiens}
On peut généraliser la notion de produit scalaire au cas des espaces vectoriels sur \(\C\), en particulier, on dira demandera alors que la forme \(f : H\times H \rightarrow \C\) soit:
\customBox{width=11cm}{
   \begin{align*}
      &\textbf{Linéaire à gauche} &&f(x + \lambda y, z) = f(x, z) + \lambda f(y, z) \\
      &\textbf{Symétrie Hermitienne} && f(u, v) = \overline{f(v, u)} \\
      &\textbf{Définie} &&f(u, u) = 0 \implies u = 0 \\
      &\textbf{Positivité} &&f(u, u) \geq 0
   \end{align*}
}   
On dira alors que \(f\) est un \textbf{produit hermitienne}. Elle est alors dite \textbf{sesquilinéaire} car on a:
\[
   f(x, \lambda y) = \overline{\lambda}f(x, y)
\]
On définit aussi pour toute matrice dans \(\mathcal{M}_n(\C)\), sa \textbf{matrice ajointe} donnée par:
\[
   M^* = {}^t\overline{M}
\]
\subsection*{\subsecstyle{Exemples {:}}}
\begin{itemize}
   \item On définit sur \(\C^n\) le produit hermitien défini par:
   \[
      \dotproduct{u}{v} = \sum_{k=1}^{n} u_k\overline{v_k}   
   \]
   \item On définit sur \(\mathcal{C}^1(\icc{0}{1}, \C)\) le produit hermitien défini par:
   \[
      \dotproduct{f}{g} = \int_{0}^{1} f(t)\overline{g(t)} d t    
   \]
   \item On définit sur \(\C_n[X]\) le produit hermitien défini pour \((x_n)\) \(n + 1\) points fixés par:
   \[
      \dotproduct{P}{Q} = \sum_{k=0}^{n}P(x_k)\overline{Q(x_k)}
   \]   
   \item On définit sur \(\mathcal{M}_n(\C)\) le produit hermitien défini par:
   \[
      \dotproduct{A}{B} = \text{tr}(AB^*)
   \]
\end{itemize}
\subsection*{\subsecstyle{Expression matricielle{:}}}
En dimension finie, on peut représenter les vecteurs \(x, y \in H\) dans une base \(\mathscr{B} = (e_i)\), on peut alors développer par sesquilinéarité:
\customBox{width=5.5cm}{
   \[
      f(x, y) = \sum_{1 \leq i, j \leq n}x_i\overline{y_j} f(e_i, e_j)   
   \]
}
En particulier on peut alors remarquer que la forme bilinéaire est parfaitement déterminée par la donnée des \(n^2\) images des paires des vecteurs de la base. En particulier, en notant \(X, Y\) les vecteurs colonnes des coordonées de \(x, y\) dans la base, alors on peut montrer qu'il existe une matrice \(A\) telle que:
\customBox{width = 4cm}{
   \(
      [f(x,y)]^\mathscr{B} = X^*A Y
   \)
}
Et cette matrice est alors de la forme suivante:
\[
   (a_{i, j}) = f(e_i, e_j)
\]
En particulier cette matrice est égale à son adjointe et représente parfaitement la forme bilinéaire symétrique et la forme quadratique\footnote[1]{Il suffit de calculer les coordonées de \(f(x, x)\) pour le voir, on trouve alors qu'elle sont égales à \(X^* A X\)}.
\subsection*{\subsecstyle{Orthogonalité{:}}}
On peut alors définir la même notion d'orthogonalité et montrer que pour un produit hermitien, toute les propriétés de l'orthogonalité sont conservées sauf une, en effet le \textbf{théorème de Pythagore} n'est plus vrai dans un espace hermitien et on a seulement:
\[
   x \perp y \implies \vectNorm{x + y}^2 = \vectNorm{x}^2 + \vectNorm{y}^2
\]
\subsection*{\subsecstyle{Théorème de représentation{:}}}
On se donne un élément \(\phi\) du dual d'un espace hermitien alors, dans ce cadre, on peut aussi montrer \textbf{le théorème de représentation de Riesz} qui caractérise une forme linéaire \(\phi\) par le produit scalaire:
\[
   \exists w \in H \; , \; \forall x \in H \; ; \; \phi(x) = \dotproduct{x}{w}   
\]
A nouveau, cela signifie que toute forme linéaire est \textbf{exactement représentée} par le produit scalaire pour un certain vecteur \(w\), en particulier, on a donc une bijection entre \(H\) et son dual.\<
\chapter*{\chapterstyle{V --- Opérateurs bornés}}
\addcontentsline{toc}{section}{Opérateurs bornés}
On s'intéresse dans ce chapitre à l'espace des applications linéaires \(\mathcal{L}(E, F)\) où \( E, F \) sont normés. C'est un exemple fondamental en analyse fonctionnelle et en mathématiques en général. On s'intéressera dans ce chapitre à la \textbf{continuité} de ces applications et à poser quelques bases de topologie sur cet espace.
\subsection*{\subsecstyle{Continuité des applications linéaires {:}}}
Soit \(f : E \rightarrow F\) un opérateur linéaire, alors on a le théorème fondamental suivant qui énonce que \(f\) est continue sur son domaine de définition si et seulement si elle vérifie une des conditions équivalentes suivantes:
\begin{itemize}
   \item Elle est continue en 0.
   \item Elle est bornée sur la boule unité.
   \item Elle est uniformément continue.
   \item Elle est lipschitzienne.
\end{itemize}
On dira alors que l'opérateur est \textbf{borné}. Muni des ces équivalences, on peut démontrer la propriétés fondamentale suivante:
\begin{center}
   \textbf{En dimension finie, tout les opérateurs linéaires sont bornés.}
\end{center}
\subsection*{\subsecstyle{Normes d'opérateur {:}}}
On peut munir \( \mathcal{L}(E) \) lui même d'une norme appelée \textbf{norme d'opérateur} d'un opérateur \(f\) par la quantité suivante:
\[
   \opNorm{f} = \inf \left\{ K \; ; \; \forall x \in E \; \vectNorm{f(x)} \leq K \right\} = \sup_{\vectNorm{x} = 1} \vectNorm{f(x)}
\]
En outre si \( E \) est de dimension finie, par l'isomorphisme usuel entre l'espace des application linéaires et celui des matrices, on définit de même la \textbf{norme d'opérateur d'une matrice} par:
\[
   \opNorm{M} = \inf \left\{ K \; ; \; \forall x \in \mathcal{M}_n(\K) \; \vectNorm{MX} \leq K \right\} = \sup_{\vectNorm{X} = 1} \vectNorm{MX}
\]
On peut alors caractériser la continuité d'un opérateur par le fait que sa norme d'opérateur soit \textbf{finie}.
\subsection*{\subsecstyle{Normes matricielles {:}}}
On appelle \textbf{norme matricielle} toute norme sur un espace de matrice qui est aussi une \textbf{norme d'algèbre}, ie telle que:
\[
   \vectNorm{AB} \leq \vectNorm{A}\vectNorm{B}
\]
Par exemple, la norme de Frobenius est une norme matricielle. On a alors la propriété\footnote[1]{Pour montrer ceci il faut remarquer que pour tout \(x \in E\), on a \(\vectNorm{f(x)} = \opNorm{f}\vectNorm{x}\).} suivante:
\begin{center}
   \textbf{Toute norme d'opérateur est une norme matricielle.}
\end{center}
\subsection*{\subsecstyle{Normes usuelles {:}}}
On peut alors chercher à savoir à quoi correspondent les normes d'opérateurs induites par les normes usuelles, on peut montrer qu'elles vérifient:
\begin{itemize}
   \item La norme 1: \(\opNorm{A}_{1} = \max_{1 \leq j \leq n} \sum_{1 \leq i \leq n} |a_{i,j}|\), c'est le \textbf{maximum de la somme des colonnes.}
   \item La norme infinie: \(\opNorm{A}_{\infty} = \max_{1 \leq i \leq n} \sum_{1 \leq j \leq n} |a_{i,j}|\), c'est le \textbf{maximum de la somme des lignes.}
   \item La norme 2: \(\opNorm{A}_{2} = \sqrt{\rho({}^tAA)}\) où \(\rho(M)\) est le \textbf{rayon spectral} de \(M\).
\end{itemize}

On peut alors chercher à savoir à quoi correspondent les normes d'opérateurs induites par les normes usuelles, on peut montrer qu'elles vérifient:
\begin{itemize}
   \item La norme 1: \(\opNorm{A}_{1} = \max_{1 \leq j \leq n} \sum_{1 \leq i \leq n} |a_{i,j}|\), c'est le \textbf{maximum de la somme des colonnes.}
   \item La norme infinie: \(\opNorm{A}_{\infty} = \max_{1 \leq i \leq n} \sum_{1 \leq j \leq n} |a_{i,j}|\), c'est le \textbf{maximum de la somme des lignes.}
   \item La norme 2: \(\opNorm{A}_{2} = \sqrt{\rho({}^tAA)}\) où \(\rho(M)\) est le \textbf{rayon spectral} de \(M\).
\end{itemize}

\chapter*{\chapterstyle{V --- Espaces de Hilbert}}
\addcontentsline{toc}{section}{Espaces de Hilbert}
Dans ce chapitre avancé, en utilisant les notions définies dans le chapitre de topologie et de théorie de la mesure, on cherche à généraliser la notion \textbf{d'orthogonalité}, de \textbf{de base orthogonale} dans le cadre d'un espace vectoriel de dimension quelconque. Pour ceci, on se donne un espace préhilbertien \( H \) à priori complexe muni de sa forme sesquilinéaire.
\begin{center}
   \textbf{On dire que \( H \) est un espace de Hilbert si il est complet pour la norme induite par le produit scalaire.}
\end{center}
En particulier, les espaces de Hilbert sont donc des espace de Banach. Dans tout la suite, on fixera la semi-linéarité du produit scalaire à gauche.
\subsection*{\subsecstyle{Exemples {:}}}
Les deux exemples canoniques d'espaces de Hilbert sont  les espaces \( \ell^2(\N) \) et \( L^2(\R) \) munis des produits scalaires respectifs suivants:
\[ 
   \dotproduct{u}{v} = \sum_{i \in \N} \overline{u_i}v_i \quad\quad\quad \dotproduct{f}{g} = \int_\R \overline{f}g d\mu
\]
En outre, en utilisant le résultat donnant que tout espace vectoriel de dimension finie est complet, on peut montrer que \( \R^n, \C^n \) et plus généralement que tout les espaces vectoriels de dimension finie sont des espaces de Hilbert pour leurs produits scalaires respectifs.
\subsection*{\subsecstyle{Orthogonalité {:}}}
On généralise alors naturellement la définition de l'orthogonalité à ce cas par:
\[ 
   \forall x, y \in H \; ; \; x \perp y \iff \dotproduct{x}{y} = 0
\]
Cette définition nous permet alors naturellement d'étendre la notion \textbf{famille orthogonale et orthonormale} ainsi que celle \textbf{d'orthogonal d'une partie}. On montre alors la propriétés fondamentale suivante pour l'orthogonal:
\begin{center}
   \textbf{C'est un sous-espace vectoriel fermé.}
\end{center}
En effet, en dimension infinie, les sous-espaces ne sont pas nécessairement fermés (contre-exemple ?). On peut montrer néanmoins que l'orthogonal l'est car il s'exprime comme l'intersection des fermés suivants:
\[ 
   A^\perp = \bigcap \text{Ker}(\dotproduct{x}{\cdot}) 
\]
\subsection*{\subsecstyle{Projection orthogonale {:}}}
On généralise la notion de projection orthogonale du cadre euclidien, et on appelera \textbf{projeté} du point \( x \in H \) sur une partie \( C \) nécéssairement \textbf{convexe et fermée} comme le point \(p_C(x) \in C\) si il existe défini par:
\[ 
   \vectNorm{p_C(x) - x} = \min\left\{ \vectNorm{c - x} \; ; \; c \in C \right\} 
\]
C'est le point de \( C \) à distance minimale avec \( x \). On peut alors montrer qu'un tel projeté \textbf{existe toujours} dans un espace de Hilbert. En outre ce point est caractérisé par le \textbf{lemme de l'angle obtus}, ie c'est l'unique point de \( C \) qui vérifie:
\[ 
   \forall x \in H \; \forall c \in C \; ; \; \text{Re}\left(\dotproduct{x - p_C(x)}{c - p_C(x)}\right) \leq 0 
\]
Ceci signifie alors que, conformément à notre intuition, l'angle entre le segment qui relie \(x\) et \( p_C(x) \) et \( x \) et n'importe quel point de \( C \) est \textbf{toujours obtus}.
\pagebreak
\subsection*{\subsecstyle{Décomposition orthogonale {:}}}
Soit \( F \) un sous-espace vectoriel \textbf{fermé} de \( H \), alors on peut montrer que dans ce cadre, on a la décomposition en somme directe suivante:
\[ 
   H = E \oplus E^\perp 
\]
Alors dans ce cas, on peut définir la projection sur chaque composante, elle est \textbf{linéaire et idempotente}, et dans ce cas elle correpond à la projection sur une partie définie plus haut. On aussi la propriété suivante du double ortogonal:
\[ 
   A^{\perp\perp} = \text{adh}(\text{Vect}(A))
\]
\subsection*{\subsecstyle{Familles totales {:}}}
On se donne une famille \( (e_i)_{i \in I} \), alors on dira que cette famille est \textbf{totale} si et seulement si on a:
\[ 
   \text{adh}\left(\text{Vect}(e_i)\right) = H  
\]
Ce concept nous permettra par la suite de définir la notion de \textbf{base orthogonale}. Plus généralement on dira qu'une partie \( A \) est totale si et seulement si \( \text{adh}\left(\text{Vect}(A)\right) = H \). On peut alors caractériser les parties totales par:
\begin{center}
   \( A \) est \textbf{totale} \( \iff A^\perp = \left\{ 0 \right\} \)
\end{center}
\uline{Exemple:} La famille \( (x^n)_{n \in \N} \) est \textbf{totale} dans l'ensemble des fonctions continues sur un segment, c'est le \textbf{théorème de Weierstrass}.

\subsection*{\subsecstyle{Théorème de Pythagore {:}}}
On se donne une famille dénombrable orthogonale \( (e_i)_{i \in D} \), alors on peut montrer la propriété suivante:
\[ 
   (e_i)_{i \in D} \text{ est \textbf{sommable}} \iff (\vectNorm{e_i})_{i \in D} \in \ell^2(D)
\]
Et dans ce cas on a le \textbf{théorème de Pythagore}:
\[ 
   \vectNorm{ \sum e_i }^2 = \sum \vectNorm{e_i}^2
\]
\subsection*{\subsecstyle{Bases Hilbertiennes {:}}}
On peut alors définir le concept de \textbf{base Hilbertienne} d'un espace de Hilbert \( H \), et on dira qu'une famille \( (e_i)_{i \in D} \) dénombrable est une \textbf{base de Hilbert} de \( H \) si et seulement si:
\begin{itemize}
   \item C'est une \textbf{famille orthogonale}.
   \item C'est une \textbf{famille totale}.
\end{itemize}
Un espace de Hilbert qui admet un partie dénombrable dense admet une base de Hilbert et on dira alors qu'il est \textbf{séparable}. On peut alors montrer les propriétés suivantes:
\begin{itemize}
   \item \textbf{Décomposition:} Si \( x \in H \), alors on peut montrer que la famille \( (\dotproduct{e_i}{x}e_i)_{i \in D} \) est sommable et que:
   \[ 
      x = \sum_{i \in D}\dotproduct{e_i}{x}e_i
   \]
   \item \textbf{Egalité de Parseval:} Par Pythagore on a alors:
   \[ 
      \vectNorm{x}^2 = \sum_{i \in D} \left|\dotproduct{e_i}{x}\right|^2
   \]
   \item \textbf{Isomorphisme des coordonées:} Il existe alors un isomorphisme qui à chaque vecteur lui associe ses coordonées:
   \[ 
      \begin{aligned}
         \phi : H &\longrightarrow \ell^2(D) \\
         x &\longmapsto \left(\dotproduct{e_i}{x}\right)_{i \in D}
      \end{aligned}
   \]
\end{itemize}
Ce théorème est le théorème fondamental des espaces de Hilbert et permet alors d'identifier tout espace de Hilbert via ses coordonées à \( \ell^2(D) \). C'est ce théorème qui donnera naissance à \textbf{l'analyse harmonique} et la théorie des séries et transformées de Fourier que l'on verra au chapitre de théorie de la mesure.
\chapter*{\chapterstyle{V --- Endomorphismes Remarquables}}
\addcontentsline{toc}{section}{Endomorphismes Euclidiens}
Aprés avoir défini les espaces euclidiens et hermitiens, on cherche maintenant à s'intéresser aux endomorphismes qui on des propriétés intéressantes en regard du produit scalaire, on sera alors amené à les définir et les étudier. Dans tout la suite, le corps de base peut être \(\K = \R\) ou \(\K = \C\) et l'espace est muni d'un produit scalaire correspondant.
\subsection*{\subsecstyle{Isométries{:}}}
Soit \(f \in \mathcal{L}(E)\), on dira que \(f\) est une \textbf{isométrie} et on note \(f \in O(E)\) si elle \textbf{préserve les angles}, ie si:
\[
   \forall x, y \in E \; ; \; \dotproduct{f(x)}{f(y)} = \dotproduct{x}{y}   
\]
On en déduit directement qu'elle \textbf{préserve aussi les longeurs}\footnote[1]{En particulier, elle est bijective et l'image d'une base orthonormée par une isométrie est toujours orthonormée.}.\<

On peut alors facilement montrer que la composée de deux isométries est une isométrie et que la réciproque l'est aussi. Aussi on peut déduire de la définition les propriétés suivantes:
\[
   \text{Sp}(f) \subseteq \{-1, 1\}   
\]
Ainsi que comme corollaire immédiat:
\[
   \text{det}(f) \in \{-1, 1\}
\]
\subsection*{\subsecstyle{Matrices orthogonales{:}}}
On considère la matrice d'une isométrie dans une base orthonormée, on a alors d'aprés l'expression matricielle du produit scalaire:
\[
   \forall X, Y \in \mathcal{M}_{n, 1}(\K) \; ; \; {}^*(MX)MY = {}^*X{}^*MMY = {}^*XY  
\]
On remarque donc \(f\) est une isométrie si et seulement si sa matrice dans une base orthonormée vérifie \({}^*M = M^{-1}\), on appelera de telles matrices \textbf{matrices unitaires} et on notera ces matrices \(\mathbb{U}_n(\K)\). On peut alors montrer que:
\begin{center}
   \textbf{Les matrices unitaires forment un sous-groupe des matrices inversibles qu'on appelle groupe unitaire.}
\end{center}
En particulier, la matrice de passage entre deux bases orthonormées est une matrice unitaire, et les colonnes d'une telle matrice sont de norme \(1\) et deux à deux orthogonales.
\subsection*{\subsecstyle{Classification des isométries{:}}}
On peut alors classifier les isométries selon leur action sur l'orientation de l'espace:
\begin{itemize}
   \item Si det\(f = 1\) on dira que l'isométrie est directe, et on note l'ensemble de ces isométries SO\((E)\), appelé \textbf{groupe spécial orthogonal}.
   \item Si det\(f = -1\) on dira que l'isométrie est \textbf{indirecte}, mais leur ensemble ne possède pas de structure particulière.
\end{itemize}
Géométriquement, les isométries directes sont donc celles qui préservent l'orientation de l'espace, c'est un sous-groupe du groupe spécial linéaire. Dans le chapitre suivant, on classifie plus précisément les isométries dans le cas d'une petite dimension.
\pagebreak
\subsection*{\subsecstyle{Adjoint d'un endomoprhisme{:}}}
On considère un endomorphisme \(f \in \mathcal{L}(E)\), alors le théorème de représentation de Riesz nous permet d'affirmer qu'il existe un unique endomorphisme \(f^*\) qui vérifie:
\[
   \forall x, y \in E \; ; \; \dotproduct{x}{f(y)} = \dotproduct{f^*(x)}{y}   
\]
On appelle alors cet endomorphisme \textbf{l'adjoint} de \(f\), en particulier si on représente \(f\) par une matrice \(M\) dans une base orthonormée, alors on défini de meme la \textbf{matrice adjointe} de \(M\) par:
\[
   \forall X, Y \in \mathcal{M}_{n, 1}(\R) \; ; \; {}^tXMY = {}^t(M^*X)Y  
\] 
On peut alors facilement montrer que dans le cas présent de matrices, l'adjoint d'un endormorphisme est simplement sa \textbf{transposée}. On verra plus tard que la notion de matrice adjointe est une généralisation de la transposition. On peut alors entrevoir le role spécial que vont jouer les matrices symétriques. A FINIR, LA TRANSPOSEE EST EXACTEMENT LADJOINT, LIEN AVEC TOUT LE RESTE A FAIRE, UNIQUE APPLICATION QUI VERFIE <f(x), y> = <x, tf(y)>, VOIR DUALITE.
\subsection*{\subsecstyle{Endomorphismes auto-adjoints{:}}}
On appelle \textbf{endomorphisme auto-adjoint} (ou encore endomorphisme symétrique) tout endomorphisme qui est égal à son adjoint et on a donc les propriétés suivantes, cas particuliers des définitions ci-dessus:
\[
   \forall x, y \in E \; ; \; \dotproduct{x}{f(y)} = \dotproduct{f(x)}{y}     
\]
Puis matriciellement dans une base ortonormée:
\[
   \forall X, Y \in \mathcal{M}_{n, 1}(\K) \; ; \; {}^*XMY = {}^*(MX)Y  
\]
En particulier dans le cas réel, le fait d'etre auto-adjoint est caractérisé par la propriété simple suivante sur la matrice de l'endomorphisme dans une base orthonormée:
\begin{center}
   \textbf{La matrice de l'endomorphisme est symétrique.}
\end{center}

L'ensemble des endomorphismes auto-adjoints, qu'on note \(\mathcal{S}(E)\) forme un \textbf{sous-espace vectoriel} de \(\mathcal{L}(E)\). Dans la suite on va étudier les propriétés de ces endomorphismes.
\subsection*{\subsecstyle{Théorème Spectral{:}}}
Une propriété fondamentale de ces endomorphismes est la suivante:
\begin{center}
   \textbf{Leurs sous-espaces propres sont orthogonaux.}
\end{center}
On peut alors énoncer un théorème puissant de réduction pour les endomorphismes auto-adjoints, en effet soit \(f\) un tel endomorphisme, alors:
\begin{center}
   \textbf{Il existe une base orthonormée formée de vecteurs propres.}
\end{center}
On a alors le corollaire matriciel pour la matrice \(M\) de \(f\) dans une base, donné par l'existance d'une matrice de passage \(P\) dans le groupe unitaire et d'une matrice diagonale \(D\) telles que:
\[
   M = PDP^*
\]
\subsection*{\subsecstyle{Technique de réduction{:}}}
En particulier, cela nous donne une nouvelle méthode pour réduire les formes bilinéaires, on peut alors diagonaliser dans une base orthonormée et obtenir une base orthogonale pour la forme, en pratique, on effectue l'algorithme suivant:
\begin{itemize}
   \item On trouve une base pour un sous-espace propre.
   \item On l'orthogonalise par Gram-Schmidt. 
\end{itemize}
\subsection*{\subsecstyle{Lien avec les formes bilinéaires{:}}}
On sait donc que les endomorphismes autoadjoints sont représentés par des matrices symétriques, on a donc la propriété suivante:
\begin{center}
   On peut associer \textbf{une forme bilinéaire} symétrique à chaque\footnote[1]{En effet soit \(f \in S(E)\), alors \(\phi(x, y) = \dotproduct{x}{f(y)}\) est une telle forme, et réciproquement Riesz nous donne que \(\phi(x, y) = \dotproduct{x}{f(y)}\) pour une certaine fonction \(f\) qui est alors un endormorphisme autoadjoint.} endomorphisme autoadjoint et \textbf{réciproquement}.
\end{center}
En particulier, les concepts relevant de l'études des formes peuvent alors se transposer dans l'étude des endomorphismes comme les sections suivantes le démontreront.
\subsection*{\subsecstyle{Endomorphismes auto-adjoints positifs{:}}}
On définit alors les endomorphismes autoadjoints \textbf{positifs} qu'on note \(\mathcal{S}^+(E)\) définis par:
\[
   \mathcal{S}^+(E) := \{ f \in \mathcal{S}(E) \; ; \; \text{Sp}(f) \subseteq \R_+ \}  
\]
Cette définition est alors équivalente à la propriété suivante:
\[
   \forall x \in E \; \dotproduct{x}{f(x)} \geq 0
\]
Les matrices symetriques positives définissent alors des formes bilinéaires symétriques positives.
\subsection*{\subsecstyle{Endomorphismes auto-adjoints définis positifs{:}}}
On définit alors enfin les endomorphismes autoadjoints \textbf{définis positifs} qu'on note \(\mathcal{S}^{++}(E)\) définis par:
\[
   \mathcal{S}^{++}(E) := \{ f \in \mathcal{S}(E) \; ; \; \text{Sp}(f) \subseteq \R_+^* \}  
\]
Cette définition est alors équivalente à la propriété suivante:
\[
   \forall x \in E\backslash\{0_E\} \; \dotproduct{x}{f(x)} > 0 
\]
Les matrices symetriques définies positives définissent alors des nouveaux produits scalaires.

\chapter*{\chapterstyle{V --- Isométries en petite dimension}}
\addcontentsline{toc}{section}{Isométries en petite dimension}
On va maintenant étudier le cas particulier des isométries dans le cas de la petite dimension, c'est à dire dans le cas où \(E\) est de dimension \(2\) ou \(3\), on introduira un outil pratique dans ce contexte qui est le \textbf{produit vectoriel} et on classifiera les isométries dans cet espace.

\subsection*{\subsecstyle{Bases directes{:}}}
Pour ce chapitre nous auront besoin du concept \textbf{d'orientation} de l'espace défini dans le chapitre sur les déterminants, en effet on appelera \textbf{base orthonormée directe} toute base ayant même orientation que la base canonique des espaces considérés. Sinon on dira que la base est \textbf{indirecte}.

\subsection*{\subsecstyle{Cas de la dimension \(2\){:}}}
Soit \(\theta\) un réel, on définit les deux matrices orthogonales suivantes:
\[
   R_\theta := \begin{pmatrix}
      \cos(\theta) & -\sin(\theta) \\
      \sin(\theta) & \cos(\theta)
   \end{pmatrix} \quad\quad 
   S_\theta := \begin{pmatrix}
      \cos(\theta) & \sin(\theta) \\
      \sin(\theta) & -\cos(\theta)
   \end{pmatrix}  
\]
On appele alors \(R_\theta\) \textbf{matrice de rotation} d'angle \(\theta\) et \(S_\theta\) est un \textbf{symétrie axiale}\footnote[1]{D'axe la bissectrice de l'angle \(\theta\), faire un dessin.}.
Alors on a le théorème fondamental suivant, pour \(f \in O(E)\) et tout base orthonormée \(\mathscr{B}\), alors il existe \(\theta\) réel tel que:
\[
   [f]_\mathscr{B} \in \bigl\{ R_\theta, S_\theta \bigl\}   
\]
En particulier si \(f\) préserve l'orientation, alors nécéssairement, \(f\) est une \textbf{rotation}. Et donc les matrices de passages entre bases orthonormées directes sont des rotations.\+
En particulier si \(f\) ne préserve pas l'orientation, alors nécéssairement, \(f\) est une \textbf{symétrie axiale}.

\subsection*{\subsecstyle{Cas de la dimension \(3\){:}}}
Dans le cas de la dimension trois, on pose \(\epsilon = \pm 1\) et on définit la matrice suivante:
\[
   M_\theta := \begin{pmatrix}
      \cos(\theta) & -\sin(\theta) & 0\\
      \sin(\theta) & \cos(\theta) & 0\\
      0 & 0 & \epsilon
   \end{pmatrix}   
\]
Alors on a le théorème fondamental suivant, pour \(f \in O(E)\) et tout base orthonormée \textbf{directe} \(\mathscr{B}\), alors il existe \(\theta\) réel tel que:
\[
   [f]_\mathscr{B} = M_\theta 
\]
La classification des isométries dans ce cas est alors plus complexes et repose sur l'étude de la dimension de l'espace des point fixes, qu'on notera \(F\), et on peut alors les classifier selon le tableau\footnote[2]{Le dernier cas peut se ramener par des propriétés trigonométriques au cas d'un rotation, en effet \(-f = R_{\theta + \pi, u}\)} ci-dessous:
\begin{center}
   \renewcommand{\arraystretch}{0.7}
   \begin{tabular}{ c |Sc |Sc |Sc }
      \(\dim(F)\) & Orientation & Matrice & Nature \\ 
      \hline
      3 & Directe & $\begin{pmatrix}1 & 0 & 0\\ 0 & 1 & 0\\ 0 & 0 & 1 \end{pmatrix}$ & Identité\\
      \hline
      2 & Indirecte & \(\begin{pmatrix}1 & 0 & 0\\ 0 & 1 & 0\\ 0 & 0 & -1 \end{pmatrix}\) & Symétrie par rapport à \(F\) \\       
      \hline
      1 & Directe & \(\begin{pmatrix}\cos(\theta) & -\sin(\theta) & 0\\ \sin(\theta) & \cos(\theta) & 0\\ 0 & 0 & 1 \end{pmatrix}\) & Rotation d'axe \(F\) \\ 
      \hline
      0 & Indirecte & \(\begin{pmatrix}\cos(\theta) & -\sin(\theta) & 0\\ \sin(\theta) & \cos(\theta) & 0\\ 0 & 0 & -1 \end{pmatrix}\) & Composée d'une symétrie et d'une rotation d'axe \(F\) 
   \end{tabular}
\end{center}
\pagebreak

\subsection*{\subsecstyle{Produit mixte{:}}}
On considère une famille de vecteurs \(u, v, w\) et deux bases orthonormées directes \(\mathscr{B}, \mathscr{B}'\) de \(E\), alors on a:
\[
   \det([u]_\mathscr{B}, [v]_\mathscr{B}, [w]_\mathscr{B}) = \det([u]_{\mathscr{B}'}, [v]_{\mathscr{B}'}, [w]_{\mathscr{B}'})
\]
On appelle alors le déterminant de cette famille de vecteur \textbf{le produit mixte} de ces trois vecteurs, et on le note:
\[
   \det([u]_\mathscr{B}, [v]_\mathscr{B}, [w]_\mathscr{B}) = [u, v, w]
\]
C'est donc le volume orienté du parallélotope formé par les trois vecteurs.

\subsection*{\subsecstyle{Produit vectoriel{:}}}
On considère une famille de vecteurs \(u, v\), et un vecteur \(x\) quelconque, alors d'aprés le théorème de représentation de Riesz, on a:
\[
   \exists w \in E \; ; \; [u, v, x] = \dotproduct{w}{x}  
\]
On appelle alors ce vecteur \textbf{produit vectoriel} de \(u\) et \(v\) et on le note \(u \times v\). On peut alors à partir des propriétés du determinant, montrer que:
\begin{itemize}
   \item Le produit vectoriel est une application \textbf{bilinéaire altérnée}.
   \item Le produit vectoriel \(u \times v\) est \textbf{orthogonal} à \(u\) et \(v\).
   \item Si \((u, v)\) est une famille orthonormée, \((u, v, u \times v)\) est une base orthonormée directe.
   \item Si \((u, v, w)\) est une base orthonormée directe \(w = u \times v, u = v \times w\) et \(v = w \times u\).
\end{itemize}
Le produit vectoriel est donc un moyen trés pratique de \textbf{construire des bases directes} ou de tester la colinéarité. Analytiquement, on peut le calculer en cordonnées par:
\[
   \begin{pmatrix}
      x_1 \\
      x_2 \\
      x_3  
   \end{pmatrix}\times 
   \begin{pmatrix}
      y_1 \\
      y_2 \\
      y_3  
   \end{pmatrix} = 
   \begin{pmatrix}
      \det\begin{pmatrix}
         x_2 & y_2 \\
         x_3 & y_3 \\
      \end{pmatrix} \\
      -\det\begin{pmatrix}
         x_1 & y_1 \\
         x_3 & y_3 \\
      \end{pmatrix} \\
      \det\begin{pmatrix}
         x_1 & y_1 \\
         x_2 & y_2 \\
      \end{pmatrix}  
   \end{pmatrix}
\]

\subsection*{\subsecstyle{Recherche des éléments caractéristiques{:}}}
Pour réussir à trouver le réel \(\theta\), on utilise alors le faire que dans une base adaptée, on a:
\[
   \text{tr}(f) = 2\cos(\theta) + 1    
\]
Et donc à partir de la trace de la matrice dans une base quelconque, on peut retrouver le cosinus de l'angle \(\theta\) dont il reste à déterminer le sinus pour le caractériser.\<

Arrive alors l'intérêt du produit mixte, en effet, on peut alors montrer que le sinus de l'angle \(\theta\) est du même signe que la quantité ci-dessous, pour \(x\) un vecteur quelconque (souvent \(c_1\)) de notre choix:
\[
   [x, f(x), u]   
\]
Ce qui caractérise alors parfaitement l'isométrie.

