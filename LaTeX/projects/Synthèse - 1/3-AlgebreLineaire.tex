\chapter*{\chapterstyle{III --- Espaces Vectoriels}} % 99% Fini
\addcontentsline{toc}{section}{Espaces Vectoriels}
L'algèbre linéaire est une partie de l'algèbre générale s'intéressant à une structure particulière omniprésente en mathématiques, la structure \textbf{d'espace vectoriel}, c'est une structure tout comme les groupes et les anneaux et elle formalise la plupart des notions géométriques usuelles dans les espaces commes \( \R^2 \) ou \( \R^3 \). Tout les résultats d'algèbre générale s'appliquent bien évidemment à cette structure.
\subsection*{\subsecstyle{Définition {:}}}
Soit \(E\) un ensemble non-vide et \(\K\) un corps commutatif. On dira alors que \((E, +, \cdot)\) est un \textbf{espace vectoriel sur \( \K \)} si les conditions ci-dessous sont réunies:
\begin{itemize}
   \item \((E, +)\) est un \textbf{groupe abélien}.
   \item La loi externe \( \cdot \) est une \textbf{action de groupe} sur \( E \) qui vérifie la \textbf{distributivité mixte}.
\end{itemize}
On appelle alors vecteurs les éléments de \(E\) et scalaires les éléments de \(\K\).
\subsection*{\subsecstyle{Sous-espaces vectoriels {:}}}
Soit \(F \subseteq E\), on dit que \(F\) est un \textbf{sous-espace vectoriel} et on note \( F \leq E \) si et seulement si:
\begin{itemize}
   \item \( F \) est non-vide.
   \item \( F \) est stable par somme.
   \item \( F \) est stable par multiplication externe.
\end{itemize}

On montre alors facilement que \textbf{l'intersection} de deux sous-espaces vectoriels est aussi un sous-espace vectoriel mais que l'union de deux sous-espaces vectoriels n'est en général pas un sous-espace vectoriel.
\subsection*{\subsecstyle{Sous-espace engendré {:}}}
On se donne une partie \( F \) de \( E \), on peut alors caractériser le \textbf{sous-espace vectoriel engendré} comme défini dans le chapitre d'algèbre par:
\[
   \text{Vect(\(F\))} := \left\{ \sum_{i=0}^{n} \lambda_i u_i\; ; \; (\lambda_{i}, u_{i}) \in \K \times F \; , \;  n \in \N  \right\}
\]
\begin{center}
    \textit{
      On dit que de telles combinaisons sont \textbf{des combinaisons linéaires} de vecteurs de \(F\). Le sous-espace engendré est donc l'ensemble des combinaisons linéaires finies de vecteurs de \( F \).
    }
\end{center}
\subsection*{\subsecstyle{Familles libre et génératrices {:}}}
Soit \(\Fam := (u_1, \ldots, u_n)\) une famille de vecteurs de \(E\).\<

On dit que \(\Fam\) est \textbf{génératrice} de E si on a \(E = \text{Vect(\(\Fam\))}\).\+
On dit que \(\Fam\) est \textbf{libre} si tout combinaison linéaire nulle de vecteurs de \(\Fam\) est à coefficient tous nuls.\<

Cette proposition signifie exactement que l'on ne peut pas obtenir un vecteur comme combinaison linéaire d'autres vecteurs, en effet si un des coefficients était non nul, il suffirait d'isoler le vecteur correspondant et il serait alors redondant. Formellement, on a:
\[
    \forall (\lambda_1, \ldots, \lambda_n) \in K^n \; ; \; \Biggr[ \sum_{i=0}^n \lambda_i u_i = 0_E \implies (\lambda_1, \ldots, \lambda_n) = (0, \ldots, 0) \Biggr]  
\]
\pagebreak
\subsection*{\subsecstyle{Bases {:}}}
On appelle \textbf{base} de \(E\) une famille \textbf{libre et génératrice}. Ce concept permet alors de caractériser le fait que tout élément de \( E \) peut s'écrire comme une \textbf{unique} combinaison linéaire des vecteurs de la base, ie si on considère \(\mathscr{B} = (e_1, \ldots, e_{n})\) une base de \(E\) on a:
\[ 
   \forall u \in E \; ; \; u = \sum_{i \leq n} u_ie_i 
\]
Pour une certaine famille \( (u_i) \), qu'on appelle alors \textbf{coordonnées} de \(u\) dans la base \(\mathscr{B}\).
\subsection*{\subsecstyle{Sommes et sommes directes{:}}}
Soit \(n\) un entier naturel et \((F_k)_{k \leq n}\) une famille finie de sous-espaces vectoriels de \(E\), Alors on peut construire le plus petit sous espace vectoriel qui contient tout les \( (F_k) \) par:
\[
   S := \sum_{k \leq n} F_k := \left\{ \sum_{k \leq n} u_k  \; ; \; u_k \in F_k \right\}
\]
On dira alors que cette somme est \textbf{directe} si et seulement si la décomposition de zéro dans la somme est \textbf{unique}\footnote[1]{En particulier si \( n=2 \), on montre directement qu'une condition nécessaire et suffisante pour que la somme soit directe est que: \[ 
   F \cap G = \left\{ 0_E \right\}  
\]}, ie:
\[ 
   \sum_{k \leq n} u_k = 0_E  \; ; \; u_k \in F_k \implies (u_1, u_2, \ldots, u_n) = (0, 0, \ldots, 0)
\]

Et on la note alors:
\[
   S = \bigoplus_{k \leq n} F_k
\]
Si de plus la somme est égale à l'espace entier, alors on dira que les \(F_k\) sont \textbf{supplémentaires} dans \(E\). 
\subsection*{\subsecstyle{Caractérisation par les bases{:}}}
Soit \(\B_k\) des bases de chacun des \(F_k\), soit la famille \(\Fam = (\B_1, \B_2, \ldots, \B_n)\), ie la famille constituée de bases des \(F_k\) concaténées. Alors on a alors le théorème suivant:
\[
   \Fam \text{ est une base de } S \Longleftrightarrow \sum_{k \leq n} F_k = \bigoplus_{k \leq n} F_n
\]
\subsection*{\subsecstyle{Hyperplans{:}}}
En particulier on définit alors le concept \textbf{d'hyperplan} comme tout espace dont le supplémentaire est une droite. Ce sont exactement les espaces définis par une équation cartésienne non triviale.\<

Un systême d'équations homogènes revient alors à déterminer l'intersection d'hyperplans.
\subsection*{\subsecstyle{Espaces vectoriels quotient{:}}}
Les espaces vectoriels étant des groupes commutatifs par définition, tout ses sous groupes sont normaux, et la compatibilité par la loi externe est directe, on peut donc définir pour tout \( F \leq E \), \textbf{l'espace vectoriel quotient} \( E/F \).
\begin{itemize}
   \item \uline{Exemple 1:} Si \( E = \R^2 \) et \( F = \text{Vect}(1, 0) \), alors \( E/F \) est \textbf{l'ensemble des droites parallèles à l'axe des abcisses}.
   \item \uline{Exemple 2:} Si \( E = \R_3[X] \) et \( F = \text{Vect}(X^2) \), alors \( E/F \) est \textbf{l'ensemble des polynômes qui ne différent que d'un terme quadratique}.
\end{itemize}
\chapter*{\chapterstyle{III --- Espaces Affines}} % 90 Fini
\addcontentsline{toc}{section}{Espaces affines}
Soit \(\mathscr{E}\) un ensemble non-vide et \(V\) un \(\R\)-espace vectoriel de dimension \(n\) finie.
\subsection*{\subsecstyle{Définition {:}}}
On appelle \textbf{espace affine\footnote[1]{Ses éléments sont alors appelés des \textbf{points}.} de direction} \(V\) le couple \((\mathscr{E}, +)\) avec:
\[
   \begin{aligned}
      + : &&\mathscr{E} \times V &\longrightarrow \mathscr{E}\\
      &&(\,A, \, u\,) &\longmapsto A + u
   \end{aligned}
\]
La loi + doit vérifer les axiomes suivants 
\footnote[2]{Une action d'un groupe \((G, \star)\) sur \(E\) est une application \(+\) de \(G \times E \) dans \(E\) qui vérifie: \[
   \forall g, g' \in G  \; , \; \forall x \in E \; ; \; x + (g \star g') = (x + g) \star g'
\] \hspace{15pt} En d'autres termes, additionner d'abord les vecteurs, ou d'abord le point avec le vecteur n'importe pas.}:
\customBox{width=13cm}{
   \begin{align*}
      &\textbf{Existence d'un neutre} && \forall A \in \mathscr{E} \; ; \; A + 0_V = A\\
      &\textbf{Action de groupe} && \forall A \in \mathscr{E} \; , \; \forall u,v \in V \; ; \; A + (u + v) = (A + u) + v \\
      &\textbf{Unicité du translaté} && \forall A, B \in \mathscr{E} \; , \; \exists !u \in V \; ; \; A + u = B 
   \end{align*}
}    

Etant donné deux points \(A, B \in \mathscr{E}\), on note alors \(\overrightarrow{AB}\) l'unique vecteur \(u\) qui vérifie:
\customBox{width=2.5cm}{
   \(A + u = B\)
}
On appelle alors \(A\) le \textbf{point initial} et \(B\) le \textbf{point final}
\subsection*{\subsecstyle{Propriétés {:}}}
Si \(\mathscr{E}\) est un espace affine de direction \(V\), on a alors les propriétés suivantes:
\customBox{width=8cm}{
   \begin{align*}
      &\textbf{Relation de Chasles} && \overrightarrow{AB} + \overrightarrow{BC} = \overrightarrow{AC} \\
      &\textbf{Existence d'un neutre} && \overrightarrow{AB} + \overrightarrow{BA} = O_V
    \end{align*}
}
\subsection*{\subsecstyle{Repères {:}}}
Un repère de \(\mathscr{E}\) est un couple \(\mathscr{R} = (O, \mathscr{B})\) formé d'un point \(O \in \mathscr{E}\) et d'une base \(\mathscr{B} = (e_1, \ldots, e_n)\) de \(V\). On appelle alors le point \(O\) \textbf{origine} du repère, et les vecteurs \(e_1, \ldots, e_n\) \textbf{vecteurs de base} du repère.\<

Soit \(i\in\inticc{1}{n}\), alors pour tout point \(A \in \mathscr{E}\), on appelle \textbf{coordonées} de \(A\) dans le repère \(\mathscr{R}\), les composantes \((x_i)_i\) du vecteur qui représente la translation de \(O\) vers \(A\) dans la base \(\mathscr{B}\), et on a la caractérisation élémentaire suivante:
\[
   \overrightarrow{OA} = x_1e_1 + \ldots + x_ne_n 
\]
\subsection*{\subsecstyle{Cas particulier des espaces vectoriels {:}}}
Il est important de noter que le couple \((V, +)\) forme un espace affine de \textbf{direction lui-même}. En effet, si on considère un élément de \(V\) comme un point, alors le couple  \((V, +)\) forme un espace affine de direction \(V\) et la loi \(+\) est alors exactement la loi de composition interne de \(V\).\<

L'unique vecteur \(u\) tel que \(A + u = B\) est exactement \(B - A\) (ici \(A, B\) sont des points de \(V\) qui se trouvent être des vecteurs dans ce cas particulier).

\chapter*{\chapterstyle{III --- Théorie de la dimension}} % Fini 99%
\addcontentsline{toc}{section}{Théorie de la dimension}

Dans ce chapitre, on considérera un espace vectoriel \(E\) qui admet une famille génératrice \textbf{finie}. On dira alors que \(E\) est de \textbf{dimension finie}.

\subsection*{\subsecstyle{Théorème de la base incomplète{:}}}
\addcontentsline{toc}{subsection}{Théorème de la base incomplète}

Soit \(\mathcal{L}\) une famille libre et \(\mathcal{G}\) une famille génératrice de \(E\), le concept de dimension se définit gràce au \textbf{théorème de la base incomplète}:
\begin{itemize}
   \item On peut \textbf{compléter} \(\mathcal{L}\) en une base de \(E\) par ajouts de vecteurs de \(\mathcal{G}\).
   \item On peut \textbf{extraire} de \(\mathcal{G}\) une base de \(E\).
\end{itemize}
Ce théorème permet alors d'assurer \textbf{l'existence} d'une base d'une espace vectoriel de dimension finie. Il se démontre par exhibition d'un algorithme qui complête \( \mathcal{L} \) en une base.
\subsection*{\subsecstyle{Théorème de la dimension{:}}}
\addcontentsline{toc}{subsection}{Théorème de la dimension}

On peut alors montrer que le cardinal d'une partie libre est toujours inférieur au cardinal d'une partie génératrice\footnote[1]{Aussi appellé \textbf{lemme de Steiniz}.}, de cette considération, on peut alors montrer directement \textbf{le théorème de la dimension} qui énonce que toutes les bases d'un espace vectoriel de dimensions fini ont \textbf{même cardinal}.\<

Ce théorème permet alors d'assurer \textbf{l'unicité} du cardinal des bases d'une espace vectoriel de dimension finie.
\subsection*{\subsecstyle{Définition de la dimension{:}}}
\addcontentsline{toc}{subsection}{Définition de la dimension}

Des deux théorèmes précédents, on a alors l'existence de bases d'une espace de dimension fini, et l'unicité de leur cardinal, on peut alors définir \textbf{la dimension d'une espace vectoriel} \(E\) comme ce cardinal et on la note \(\text{dim}(E)\).
\subsection*{\subsecstyle{Espaces de dimension finie{:}}}
\addcontentsline{toc}{subsection}{Espaces de dimension finie}

Considérons maintenant \(E\) un espace vectoriel de dimension finie \(n\) est \(\Fam\) une famille de \(n\) vecteurs de \(E\). Alors par déduction immédiate de la définition de dimension, on a:
\begin{center}
   \(\Fam\) est libre \(\Longleftrightarrow\) \(\Fam\) est génératrice \(\Longleftrightarrow\) \(\Fam\) est une base.
\end{center}
Soient \(F, G\) deux sous-espaces de \(E\). La dimension permet aussi de prouver des \textbf{égalités} d'espaces vectoriels, gràce aux propriétés suivantes:
\begin{itemize}
   \item Si \( F \subseteq G \) et \( \text{dim}(F) = \text{dim}(G) \), alors \( F = G \).
   \item Si \(F, G\) sont en \textbf{somme directe} et que \( \text{dim}(F) + \text{dim}(G) = \text{dim}(E) \), alors ils sont \textbf{supplémentaires}.
\end{itemize}
Enfin on peut calculer la dimension d'une somme avec la \textbf{formule de Grassmann}:
\[ 
   \text{dim}(F + G) = \text{dim}(F) + \text{dim}(G) - \text{dim}(F \cap G)
\]
\subsection*{\subsecstyle{Rang d'une famille de vecteurs{:}}}
\addcontentsline{toc}{subsection}{Rang d'une famille de vecteurs}
Soit \(E\) un espace vectoriel de dimension finie et \(\Fam\) une famille de vecteurs de cet espace, alors on appelle \textbf{rang} de \(\Fam\) l'entier:
\[ 
   \text{rg}(\Fam) = \text{dim}(\text{Vect}(\Fam))
\]
C'est simplement \textbf{la dimension du sous-espace engendré par la famille}. D'aprés la section ci-dessus on a donc une caracation des bases, en effet:
\[ 
   \mathscr{F} \text{ est une base } \iff \text{rg}(\mathscr{F}) = n 
\]
\chapter*{\chapterstyle{III --- Applications Linéaires}} % Fini 99%
\addcontentsline{toc}{section}{Applications linéaires}

Soit deux \(\K\)-espaces vectoriels \(E\) et \(F\), et \(f : E \longrightarrow F \).

\subsection*{\subsecstyle{Définition{:}}}
\addcontentsline{toc}{subsection}{Définition}

On dit que \(f\) est une \textbf{application linéaire} si c'est un \textbf{morphisme d'espaces vectoriels}, ie si et seulement si pour tout couple de vecteurs \(u, v \in E\) et pour tout scalaire \(\lambda\) elle vérifie: 
\begin{itemize}
   \item \textbf{ Loi  interne : } \( f(u + v) = f(u) + f(v) \)
   \item \textbf{ Loi  externe : } \( f(\lambda u) = \lambda f(u) \)
\end{itemize}
On note alors \(\mathcal{L}(E, F)\) l'ensemble des applications linéaires de \(E\) vers \(F\). Si \( F = \K \), on dira que \( f \) est une \textbf{forme linéaire}.
\subsection*{\subsecstyle{Propriétés{:}}}
\addcontentsline{toc}{subsection}{Propriétés}

On s'intéresse au propriétés de l'ensemble \(\mathcal{L}(E, F)\), c'est un ensemble de morphismes donc d'aprés le chapitre d'algèbre la composée de morphismes et l'inverse d'un morphisme bijectif est un morphisme.\<

En outre en considèrant les espaces vectoriels comme groupes additifs, on vérifie que le noyau d'un morphisme est un sous-espace bien défini et caractérise l'injectivité de ce dernier. De même, l'image de générateurs engendre l'image qui est aussi un sous-espace.
\subsection*{\subsecstyle{Caractérisations par les familles{:}}}
\addcontentsline{toc}{subsection}{Caractérisations par les familles}

Soit \(\mathscr{F} = (e_i)_{i \in \N}\) une famille de \(E\) et un endomorphisme de \(E\), alors \(f\) est entièrement caractérisée par l'image de cette famille, en effet on a:
\begin{itemize}
   \item L'image d'une famille libre est libre \(\Longleftrightarrow f\) est injective.
   \item L'image d'une famille génératice est génératice \(\Longleftrightarrow f\) est surjective.      
\end{itemize}
\subsection*{\subsecstyle{Endomorphismes élémentaires remarquables{:}}}
\addcontentsline{toc}{subsection}{Endomorphismes élémentaires remarquables}
On définit ici des endomorphismes élémentaires remarquables comme:
\begin{itemize}
   \item \textbf{Les homotéthies:} Elles sont caractérisées par \(\varphi_k : u \longmapsto ku\)
   \item \textbf{Les symétries:}Elles sont caractérisées par \(\varphi \circ \varphi = \text{Id}_E\)
   \item \textbf{Les projecteurs:} Elles sont caractérisées par \(\varphi \circ \varphi = \varphi\)
\end{itemize}
Précisons le cas des projecteurs, en effet si on considère deux sous-espaces \(F, G\) supplémentaires dans \(E\), alors chaque élément \(u \in E\) admet une décomposition unique de la forme \(u = v_F + v_G\). Cette décomposition définit canoniquement deux projecteurs, par exemple celui de direction \(F\) sur \(G\) qui est l'application \(\varphi\) telle que:
\[
   \varphi(u) = v_G
\]
Graphiquement, pour \(\varphi\) le projecteur de direction \(F\) sur \(G\):
\begin{center}
   \begin{tikzpicture}[yscale=0.98]
      \draw[black!15] (-4,0) -- (4,0);
      \draw[black!15] (0, -2) -- (0,2);

      \draw[color=BrightBlue1, domain=-0.5:0.5, thick] plot (\x,{4*\x}) node[left] {$F$};
      \draw[color=BrightBlue1, domain=-4:4, thick] plot (\x,{0.5*\x}) node[below right] {$G$};

      \draw[-latex, color=BrightRed1, thick] (0, 0) -- (3/14, 6/7) node[left] {$v_F$};
      \draw[-latex, color=BrightRed1, thick] (0, 0) -- (9/7, 9/14) node[below right] {$v_G = \varphi(u)$};

      \draw[color=BrightRed1, dashed] (9/7, 9/14) -- (1.5, 1.5);
      \draw[color=BrightRed1, dashed] (3/14, 6/7) -- (1.5, 1.5);

      \draw[-latex, color=DarkBlue1, thick] (0, 0) -- (1.5,1.5) node[above left] {$u$};
   \end{tikzpicture} 
\end{center}
\subsection*{\subsecstyle{Applications linéaires en dimension finie{:}}}
\addcontentsline{toc}{subsection}{Applications linéaires en dimension finie}
On étends la définition de rang d'une famille à celle du \textbf{rang d'une application linéaire}, qu'on note \(\text{rg}(f)\), qui correspond à \textbf{la dimension de son image}. Soit \(f \in \mathcal{L}(E, F)\) une application de rang fini, alors d'après le premier théorème d'isomorphisme:
\[ 
   E/\text{ker}(f) \cong \text{Im}(f) 
\]
Et donc on a égalité des dimensions et aprés avoir montré que \( \text{dim}(E/F) = \text{dim}(E) - \text{dim}(F) \), on en déduis le \textbf{théorème du rang}:
\[ 
   \text{dim}(E) = \text{rg}(f) + \text{dim}(\text{Ker}(f))
\]
Ce thèorème permet alors de caractériser l'injectivité et la surjectivité d'une application linéaire par:
\begin{itemize}
   \item \(f\) est injective si et seulement si \(\text{rg}(f) = \text{dim}(E)\)
   \item \(f\) est surjective si et seulement si \(\text{rg}(f) = \text{dim}(F)\)
\end{itemize}
En particulier, si \(E\) et \(F\) sont deux espaces vectoriels \textbf{de même dimension} alors:
\begin{center}
   \(f\) est injective \(\Longleftrightarrow\) \(f\) est surjective \(\Longleftrightarrow\) \(f\) est bijective
\end{center}
Ceci caractérise alors \textbf{les isomorphismes en dimension finie}.
\subsection*{\subsecstyle{Applications multilinéaires{:}}}
\addcontentsline{toc}{subsection}{Applications multilinéaires}

On peut généraliser le concept de linéarité à celui de \textbf{multilinéarité} ou \textbf{n-linéarité}. On considère alors une application de la forme \(f : E^n \longrightarrow F\), alors on dit que \(f\) est multilinéaire si et seulement si elle est linéaire \textbf{en chacune des variables}, ie si:
\begin{align*}
   f(e_1 + \lambda u, e_2, \ldots, e_n) &= f(e_1, e_2, \ldots, e_n) + \lambda f(u, e_2, \ldots, e_n)  \\
   f(e_1, e_2  + \lambda u, \ldots, e_n) &= f(e_1, e_2, \ldots, e_n) + \lambda f(e_1, u, \ldots, e_n) \\
   & \vdotswithin{=} \\   
   f(e_1, e_2 , \ldots, e_n + \lambda u) &= f(e_1, e_2, \ldots, e_n) + \lambda f(e_1, e_2, \ldots, u)
\end{align*}
Une telle application est dite:
\begin{itemize}
   \item \textbf{Symétrique} si permuter deux variables préserve le résultat.\footnote[1]{\underline{Exemple:} \(f(e_1, e_2) = f(e_2, e_1)\)}
   \item \textbf{Antisymétrique} si permuter deux variables change le signe du résultat.\footnote[2]{\underline{Exemple:} \(f(e_1, e_2) = -f(e_2, e_1)\), en particulier, le signe du résultat aprés une permutation \(\sigma\) dépends alors de \textbf{la signature de la permutation} (la parité du nombre de permutations effectuées).}
   \item  \textbf{Altérnée} si elle s'annule à chaque fois qu'on l'évalue sur un k-uplet contenant deux vecteurs identiques.\footnote[3]{\underline{Exemple:} \(f(e_1, e_1) = 0\)}
\end{itemize}
\chapter*{\chapterstyle{III --- Espace des matrices}} % Fini 99%
\addcontentsline{toc}{section}{Espace des matrices}

On appelle \textbf{matrice} à \(n\) lignes et \(p\) colonnes à coefficients dans un anneau \(\mathbb{A}\) toute application de la forme:
\[
   M : \inticc{1}{n} \times \inticc{1}{p} \longrightarrow \mathbb{A}
\]
Il s'agit d'une généralisation du concept de suite sous forme de suite à \textbf{deux indices}, qu'on peut alors voir comme un tableau de nombres tel qu'en chaque position \((i, j)\), on ait un élément \(a_{ij} \in \mathbb{A}\). A l'instar des suites, on notera \(M = (a_{ij})\) pour faire référence à la matrice \(M\).\<

On note alors \(\mathcal{M}_{n, p}(\mathbb{A})\) l'espaces des matrices à \( n \) lignes et \( p \) colonnes à coefficients dans \( \mathbb{A} \).

\subsection*{\subsecstyle{Structure{:}}}
\addcontentsline{toc}{subsection}{Structure}
On peut alors munir ces espaces des opérations suivantes:
\begin{itemize}
   \item On définit \textbf{la somme} de deux matrices par la matrice obtenue en sommant par composantes.
   \item On définit \textbf{la multiplication} d'une matrice par un scalaire comme la matrice dont tout les termes sont multipliés par ce scalaire.
   \item On définit \textbf{la multiplication} de deux matrices \(A = (a_{i, j}) \in \mathcal{M}_{n, m}(\K)\) et \(B = (b_{k, j}) \in \mathcal{M}_{m, p}(\K)\) compatibles\footnote[1]{Il faut que le nombre de colonnes de la première soit égal au nombre de lignes de la seconde pour que les matrices soient compatibles.} par la matrice \(C := AB = (c_{i,j})\) comme étant la matrice telle que:
   \[
      c_{i, j} = \sum_{k=1}^{n}a_{i, k}b_{k, j}
   \]
\end{itemize} 
En particulier, on appelle ce produit \textbf{un produit ligne par colonne} qui se comprends visuellement\footnote[2]{Le coefficient à \color{BrightRed1} la troisième ligne, première colonne \color{black} est obtenu en multipliant \color{BrightBlue1} la troisième ligne par la première colonne.\color{black}} par:
\begin{center}
   \begin{tikzpicture}[    
      every left delimiter/.style={xshift=.55em},
      every right delimiter/.style={xshift=-.55em}
    ]
      \matrix[
        matrix of math nodes, ampersand replacement=\&,
        left delimiter=(, right delimiter=),  outer sep = 0pt,inner sep=4.5pt
      ](A){
        a_{1, 1} \& a_{1, 2}\\ 
        a_{2, 1} \& a_{2, 2}\\ 
        a_{3, 1} \& a_{3, 2}\\  
      };
      \draw[color=BrightBlue1, line width=0.5mm] (A-3-1.south west) rectangle (A-3-2.north east);
      \draw node at (1.5, 0) {\(\times\)};
  
      \matrix[
        matrix of math nodes, ampersand replacement=\&,
        left delimiter=(, right delimiter=),  outer sep = 0pt,inner sep=4.5pt
      ] (B) at (3.4, 0) {
        b_{1, 1} \& b_{1, 2} \& b_{1, 3}\\ 
        b_{2, 1} \& b_{2, 2} \& b_{2, 3}\\ 
      };
      \draw[color=BrightBlue1, line width=0.5mm] (B-1-1.north west) rectangle (B-2-1.south east);
  
      \draw node at (5.25, 0) {\(=\)};
  
      \matrix[
        matrix of math nodes, ampersand replacement=\&,
        left delimiter=(, right delimiter=),  outer sep = 0pt,inner sep=4.5pt
      ] (C) at (7.1, 0) {
        c_{1, 1} \& c_{1, 2} \& c_{1, 3}\\ 
        c_{2, 1} \& c_{2, 2} \& c_{2, 3}\\ 
        c_{3, 1} \& c_{3, 2} \& c_{3, 3}\\
      };
      \draw[color=BrightRed1, line width=0.5mm] (C-3-1.north west) rectangle (C-3-1.south east);
    \end{tikzpicture}
\end{center}
En particulier si \( n = p \), et que \( \mathbb{A} = \K \) est un corps, le cas le plus courant, alors toutes les matrices en jeu sont carrées et cette loi est \textbf{interne}.\<

On peut alors montrer que \( (\mathcal{M}_n(\mathbb{\K}), +, \times)\) est un \textbf{anneau unitaire} (mais ni commutatif ni intègre). Le neutre pour la multiplication est alors la matrice identité nulle partout et dont les termes diagonaux sont tous égaux à 1.
\subsection*{\subsecstyle{Matrices inversibles{:}}}
\addcontentsline{toc}{subsection}{Matrices inversibles}
On dira qu'une matrice \( M \in \mathscr{M}_n( \K) \) est \textbf{inversible} si elle admet un inverse pour la multiplication matricielle, en outre on appelle l'ensemble des matrices carrées inversibles de taille \(n\) \textbf{le groupe linéaire} d'ordre \(n\), qu'on note GL\(_n(\K)\), c'est un groupe pour la multiplication matricielle.\<

Et en particulier, on montre facilement que \((AB)^{-1} = B^{-1}A^{-1}\). Dans le cas d'une matrice inversible, on peut alors étendre notre définition de puissance d'une matrice au cas d'entiers négatifs.\<

On donnera aussi par la suite une interprétation géométrique de ce groupe et plusieurs méthodes efficaces pour prouver l'existence d'inverses et les calculer.
\subsection*{\subsecstyle{Transposition{:}}}
\addcontentsline{toc}{subsection}{Transposition}

Soit \(M = (x_{i,j})\in \mathcal{M}_{n, m}(\K)\), on définit \textbf{l'opération de transposition} d'une matrice notée \(M^\mathsf{T}\), c'est \textbf{une application linéaire involutive} définie par:
\[ 
   M^\mathsf{T} = (x_{j,i})
\]
Intuitivement, cette application transforme \textbf{chaque ligne en colonne et inversement}. Par exemple:
\[
   \begin{pmatrix}
      a & b & c\\
      d & e & f
   \end{pmatrix}^\top = 
   \begin{pmatrix}
      a & d\\
      b & e\\
      c & f
   \end{pmatrix}
\]
Il faut aussi noter son comportement par rapport au produit matriciel de deux matrices \(A, B\), précisément on a:
\[ 
   (AB)^\top = B^\top A^\top
\]
Enfin, on peut montrer que la transposition est compatible avec l'inversion, ie on a:
\[ 
   (M^{-1})^{\top} = (M^{\top})^{-1} 
\] 
\subsection*{\subsecstyle{Trace{:}}}
\addcontentsline{toc}{subsection}{Trace}

On définit aussi une autre application linéaire appellée \textbf{trace d'une matrice}, et qui est définie comme \textbf{la somme des éléments diagonaux}. Formellement:
\[
   \text{tr}(A) := \sum_{i=1}^{n} a_{i, i}   
\]
Elle est donc linéaire mais on a aussi par calcul direct:
\[ 
   \text{tr}(AB) = \text{tr}(BA)  
\]

\subsection*{\subsecstyle{Application coordonées{:}}}
Si on fixe une base \( \mathscr{B} \) de \( E \), alors tout vecteur \( u \in E \) admet des coordonées dans cette base et donc on peut définir l'isomorphisme suivant:
\[ 
   \begin{aligned}
      [ \cdot ]_{ \mathscr{B}} : E &\longrightarrow \mathscr{M}_{n, 1}( \K) \\
      u &\longmapsto \begin{pmatrix}
      u_1\\
      \vdots\\
      u_n
      \end{pmatrix} 
   \end{aligned} 
\]
On peut alors identifier ainsi les vecteurs de \( E \) avec leurs cordonnées, mais non canoniquement, ie cette représentation dépends de la base fixée. En outre on a une action naturelle de \( \mathscr{M}_n( \K) \) sur les vecteurs colonnes définie par \( M \cdot X = MX \). De cette manière on peut déja intuiter que la multiplication matricielle se comporte comme une transformation (linéaire) des coordonées des vecteurs de l'espace.
\subsection*{\subsecstyle{Matrice d'une famille de vecteurs{:}}}
\addcontentsline{toc}{subsection}{Matrice d'une famille de vecteurs}

Soit \(\mathscr{F} = (e_i)_{i \leq k}\) une famille de \( k \) vecteurs et \( \mathscr{B} \) une base de \( E \), alors on peut définir \textbf{la matrice de la famille} dans la base par:
\[
   \text{Mat}_{\mathscr{B}}(\mathscr{F}) = \left([e_1]_{\mathscr{B}}, [e_2]_{\mathscr{B}}, \ldots, [e_n]_{\mathscr{B}} \right)  
\]
C'est la matrice des coordonées des vecteurs de la famille. 
\subsection*{\subsecstyle{Espaces des colonnes et des lignes{:}}}
Réciproquement, pour toute matrice \( M = (C_1, \ldots, C_n) = (L_1, \ldots, L_p)\), on peut voir cette matrice comme la matrice d'une famille de vecteurs (ou la transposée d'une telle matrice), et on définit alors:
\begin{itemize}
   \item L'espaces des colonnes de \( M \) par \( \text{Vect}(C_1, \ldots, C_n) \)
   \item L'espaces des lignes de \( M \) par \( \text{Vect}(L_1, \ldots, L_n) \)
\end{itemize}
Ces espaces sont alors des sous espaces de \(\mathscr{M}_{n, 1}( \K) \cong \K^n \) ou de \(\mathscr{M}_{p, 1}( \K) \cong \K^p\) et seront particulièrement importants car des méthodes matricielles trés puissantes nous permettront d'étudier des familles par leur représentation matricielle.
\pagebreak
\subsection*{\subsecstyle{Matrice d'une application linéaire{:}}}
\addcontentsline{toc}{subsection}{Matrice d'une application linéaire}

Soit une application linéaire \( f : E \rightarrow F \) telle que \(\mathscr{B} = (e_i)_{i \leq n}\) est une base de \( E \) et \(\mathscr{C}=(f_i)_{i \leq p}\)). Alors on peut associer à l'application \(f\) \textbf{une unique matrice} dans les bases \(\mathscr{B, C}\), qu'on note alors Mat\(_{\mathscr{B, C}}(f)\) qu'on construit comme suit:
\[ 
   \text{Mat}_{\mathscr{B, C}}(f) = \left([f(e_1)]_{\mathscr{C}}, [f(e_2)]_{\mathscr{C}}, \ldots, [f(e_n)]_{\mathscr{C}}\right)
\]
\underline{Exemple:} Considérons un endomorphisme de \(\R^3\) et sa base canonique notée \((e_1, e_2, e_3)\), telle que \(f(x, y, z) = (2x+1, 3x, z-1)\). Alors on calcule l'image des vecteurs de la base de départ, ie:
\begin{multicols}{3}
\begin{itemize}
   \item \(f(1, 0, 0) = (3, 3, -1)\)
   \item \(f(0, 1, 0) = (0, 0, -1)\)
   \item \(f(0, 0, 1) = (0, 0, -2)\)
\end{itemize}
\end{multicols}
Puis on calcule les coordonées de ces vecteurs dans la base d'arrivée, et on les range en colonne dans une matrice et on obtient:
\begin{center}
   \begin{tikzpicture}[      
      every left delimiter/.style={xshift=.55em},
      every right delimiter/.style={xshift=-.55em}
    ]
    \draw node[color = BrightBlue1] at (-1.05, 1.75) {$f(e_1)$};
    \draw[-latex, dashed, thick, color = BrightBlue1!50] (-1.05, 1.5) -- (-1.05, 1.1);
  
    \draw node[color = BrightBlue1] at (0, 1.75) {$f(e_2)$};
    \draw[-latex, dashed, thick, color = BrightBlue1!50] (0, 1.5) -- (0, 1.1);
  
    \draw node[color = BrightBlue1] at (1.05, 1.75) {$f(e_3)$};
    \draw[-latex, dashed, thick, color = BrightBlue1!50] (1.05, 1.5) -- (1.05, 1.1);
  
    \draw node[color = BrightBlue1] at (2.3, 0.85) {$f_1$};
    \draw[-latex, dashed, thick, color = BrightBlue1!50] (1.7, 0.85) -- (2.1, 0.85);
  
    \draw node[color = BrightBlue1] at (2.3, 0) {$f_2$};
    \draw[-latex, dashed, thick, color = BrightBlue1!50] (1.7, 0) -- (2.1, 0);
  
    \draw node[color = BrightBlue1] at (2.3, -0.85) {$f_3$};
    \draw[-latex, dashed, thick, color = BrightBlue1!50] (1.7, -0.85) -- (2.1, -0.85);
  
    \matrix[
      matrix of math nodes, ampersand replacement=\&,
      left delimiter=(, right delimiter=), inner ysep=4pt, column sep=0.8em, row sep = 0.8em,
      nodes={
        inner sep=0.5em,outer sep=0,text width=1.2em,align=center
      }
    ]{
      3 \& 0 \& 0\\ 
      3 \& 0 \& 0\\ 
      -1 \& -1 \& -2\\
    };
    \end{tikzpicture}
\end{center}
On peut alors reformuler ce résultat par le fait que l'application ci-desosus est une bijection:
\[ 
   \text{Mat}_{\mathscr{B, C}} : \mathcal{L}(E, F) \longrightarrow \mathcal{M}_{n, p}(\K) 
\]
\subsection*{\subsecstyle{Propriété de morphisme{:}}}
On peut alors montrer que cette bijection est bien plus riche qu'une simple bijection, en particulier \textbf{c'est un isomorphismes d'anneaux} pour les structures ci-dessous:
\[ 
   \text{Mat}_{\mathscr{B, C}} : (\mathcal{L}(E, F), +, \circ) \longrightarrow (\mathcal{M}_{n, p}(\K), +, \times)
\]
En effet, soit \( f \in \mathcal{L}(F, G)\) et \( g \in \mathcal{L}(E, F)\) avec \( \mathscr{B}, \mathscr{C}, \mathscr{D} \) des bases de réciproquement \( E, F, G \). Alors on peut montrer la propriété suivante qui caractérise alors la matrice d'une composée, et motive la construction artificielle du produit matriciel:
\[ 
   \text{Mat}(f \circ g)_{\mathscr{B}, \mathscr{D}} = \text{Mat}_{\mathscr{C}, \mathscr{D}}(f) \times \text{Mat}_{\mathscr{B}, \mathscr{C}}(g)  
\]
En d'autres termes, la matrice de la composée est le produit des matrices.
\subsection*{\subsecstyle{Action naturelle{:}}}
Alors, appliquer une application linéaire à un vecteur \(u \in E\) revient exactement à faire agir la matrice de \( f \) pour l'action naturelle sur les vecteurs coordonées, ie on a:
\[ 
   [f(u)]_{\mathscr{C}} = \text{Mat}_{\mathscr{B}, \mathscr{C}}(f)\times [u]_{\mathscr{C}}
\]

\underline{Exemple:} Soit \(f\) de matrice 
\(M = \begin{pmatrix}
   1 & 2\\
   3 & 4\\
\end{pmatrix}\) 
et \(u = (5, 3)\), calculer les coordonées de \(f(u)\) revient à calculer:
\[
   \begin{pmatrix}
      1 & 2\\
      3 & 4\\
   \end{pmatrix}
   \begin{pmatrix}
      5 \\
      3 \\
   \end{pmatrix} =
   \begin{pmatrix}
      11 \\
      27 \\
   \end{pmatrix} 
\]
Enfin l'isomorphisme entre ces deux espaces nous permet de définir \textbf{le noyau et l'image d'une matrice}, comme simplement étant le noyau ou l'image de l'application linéaire associée à cette matrice.
\subsection*{\subsecstyle{Rang d'une matrice{:}}}
\addcontentsline{toc}{subsection}{Rang d'une matrice}

On a défini précédemment le concept de rang d'une application linéaire comme la dimension de son image, et en particulier comme la dimension du sous-espace engendré par les images des vecteurs de base \( (f(e_i))_{i \leq n} \), on peut étendre cette définition au \textbf{rang d'une matrice}, en effet on a simplement:
\begin{center}
   \textit{Le rang d'une matrice est la dimension de l'espace de ses colonnes.}
\end{center}

On remarque donc que le rang d'une matrice nous donne \textbf{énormément d'informations} sur la matrice, la famille de vecteurs de l'espace des colonnes, et l'application linéaire associée. En particulier, on peut donc caractériser les matrices inversibles et les bases par la maximalité du rang de la matrice associée.
\subsection*{\subsecstyle{Matrice de passage{:}}}
\addcontentsline{toc}{subsection}{Matrice de passage}

On considère un espace vectoriel \(E\) et deux bases \(\mathscr{B} =(e_1, \ldots, e_n), \mathscr{B}'=({e}_1', \ldots, {e}_n')\) de cet espace.\+
On appelle \textbf{matrice de passage} de \(\mathscr{B}\) à \(\mathscr{B}'\) la matrice ci-dessous:
\[ 
   \text{Pass}(\mathscr{B}, \mathscr{B}') = ([e_1']_{\mathscr{B}}, [e_2']_{\mathscr{B}}, \ldots, [e_n']_{\mathscr{B}}) 
\]

\begin{center}
   \textit{
      La matrice de passage est donc constituée des coordonées dans l'ancienne base des vecteurs de la nouvelle base(en colonnes).
   }
\end{center}
Une matrice de passage \( \text{Pass}(\mathscr{B}, \mathscr{B}') \) est alors \textbf{nécessairement inversible} d'inverse la matrice de passage \( \text{Pass}(\mathscr{B}', \mathscr{B}) \)
\subsection*{\subsecstyle{Utilisations{:}}}
Pour tout vecteur \( u \in E \), cette matrice permet alors de représenter ce vecteur dans une nouvelle base par:
\[ 
   [u]_{ \mathscr{B}'} = \text{Pass}(\mathscr{B}', \mathscr{B})[u]_{\mathscr{B}} 
\]

On peut aussi vouloir chercher à représenter un endomorphisme \( f \) dans une base différente, et en effet on peut montrer qu'on a:
\[ 
   \text{Mat}_{ \mathscr{B}'}(f) = \text{Pass}(\mathscr{B}', \mathscr{B}) \text{Mat}_{\mathscr{B}}(f) \text{Pass}(\mathscr{B}, \mathscr{B}')
\]
La notation fait alors sens de manière générale car \( \text{Pass}(\mathscr{B}, \mathscr{B}') \) prends en entrée une colonne de cordonnées dans la base \(\mathscr{B}'\) et renvoie celui dans la base précédente qui peut alors être évalué par la matrice et être retransformé par la suite.\<

De manière générale, on notera souvent \(U, U'\) le vecteurs dans la base initiale (resp. dans la base finale), \( M, M' \) la matrice dans la base initiale (resp. dans la base finale) et \( P = \text{Pass}( \mathscr{B}, \mathscr{B}') \), alors on a les formules suivantes:
\[ 
   U' = P^{-1}U \quad\quad\quad\quad\quad\quad M' = P^{-1}MP
\]
\subsection*{\subsecstyle{Matrices semblables{:}}}
\addcontentsline{toc}{subsection}{Matrices semblables}

On remarque alors plus clairement que le fait de changer la représentation d'une matrice correspond à \textbf{une action par conjugaison} de \( GL_n(\K) \), on appelle alors les classes de conjugaisons \textbf{classes de similitude} et on dira que deux éléments dans la même classe sont \textbf{semblables}. 
\begin{center}
   \textit{Ces deux matrices représentent alors le même endomorphisme, la même transformation géométrique, dans des bases différentes.}
\end{center}
En particulier on appelle alors \textbf{invariants de similitude} les propriétés qui ne dépendent pas du choix du représentant, on a alors plusieurs invariants:
\begin{itemize}
   \item La dimension du noyau.
   \item La dimension de l'image, ie le rang.
   \item La trace.
   \item D'autres invariants définis plus tard comme le déterminant, le polynôme caractéristique et minimal.
\end{itemize}   
\pagebreak
\subsection*{\subsecstyle{Matrices remarquables{:}}}
\addcontentsline{toc}{subsection}{Matrices remarquables}

Dans ce chapitre nous allons présenter brièvement différentes matrices remarquables:
\begin{figure}[h]
   \color{DarkBlue1}
   \centering
   \begin{subfigure}{.3\textwidth}
      \centering
      \begin{tikzpicture}[line cap=round]
         \matrix[
            matrix of math nodes, ampersand replacement=\&,
            left delimiter=(, right delimiter=)
         ]{
            a \& 0 \& 0\\ 
            0 \& d \& 0\\ 
            0 \& 0 \& f\\
         };
      \end{tikzpicture}
      \caption*{\color{DarkBlue1}
      Matrice diagonale}
   \end{subfigure}\quad
   \begin{subfigure}{.3\textwidth}
      \centering
      \begin{tikzpicture}[line cap=round]
         \matrix[
            matrix of math nodes, ampersand replacement=\&,
            left delimiter=(, right delimiter=)
         ]{
            a \& b \& c\\ 
            0 \& d \& e\\ 
            0 \& 0 \& f\\
         };
      \end{tikzpicture}
      \caption*{\color{DarkBlue1}
      Matrice triangulaire supérieure}
   \end{subfigure}\quad
   \begin{subfigure}{.3\textwidth}
      \centering
      \begin{tikzpicture}[line cap=round]
         \matrix[
            matrix of math nodes, ampersand replacement=\&,
            left delimiter=(, right delimiter=)
         ]{
            a \& 0 \& 0\\ 
            b \& d \& 0\\ 
            c \& e \& f\\
         };
      \end{tikzpicture}
      \caption*{\color{DarkBlue1}
      Matrice triangulaire inférieure}
   \end{subfigure}
\end{figure}

Puis on a deux types de matrices qui sont liées à la symétrie des coefficients et à \textbf{l'opération de transposition}:
\begin{figure}[h]
   \color{DarkBlue1}
   \centering
   \begin{subfigure}{.3\textwidth}
      \centering
      \begin{tikzpicture}[line cap=round]
         \matrix[
            matrix of math nodes, ampersand replacement=\&,
            left delimiter=(, right delimiter=)
         ]{
            a \& d \& f\\ 
            d \& b \& e\\ 
            f \& e \& c\\
         };
      \end{tikzpicture}
      \caption*{\color{DarkBlue1}Matrice symétrique}
   \end{subfigure}\quad
   \begin{subfigure}{.3\textwidth}
      \centering
      \begin{tikzpicture}[line cap=round]
         \matrix[
            matrix of math nodes, ampersand replacement=\&,
            left delimiter=(, right delimiter=)
         ]{
            0 \& -a \& -b\\ 
            a \& 0 \& -c\\ 
            b \& c \& 0\\
         };
      \end{tikzpicture}
      \caption*{\color{DarkBlue1}Matrice antisymétrique}
   \end{subfigure}
\end{figure}

On a aussi \textbf{les matrices élémentaires} qui représentent les \textbf{opérations élémentaires}\footnote[1]{Voir la section suivante.}:
\begin{figure}[h]
   \color{DarkBlue1}
   \centering
   \begin{subfigure}{.3\textwidth}
      \centering
      \begin{tikzpicture}[line cap=round]
         \matrix[
            matrix of math nodes, ampersand replacement=\&,
            left delimiter=(, right delimiter=)
         ]{
            1 \& 0 \& 0\\ 
            0 \& 1 \& 0\\ 
            0 \& 0 \& \lambda\\
         };
      \end{tikzpicture}
      \caption*{\color{DarkBlue1}Matrice de dilatation}
   \end{subfigure}\quad
   \begin{subfigure}{.3\textwidth}
      \centering
      \begin{tikzpicture}[line cap=round]
         \matrix[
            matrix of math nodes, ampersand replacement=\&,
            left delimiter=(, right delimiter=)
         ]{
            1 \& 0 \& 0\\ 
            0 \& 1 \& 0\\ 
            0 \& \lambda\& 1\\
         };
      \end{tikzpicture}
      \caption*{\color{DarkBlue1}Matrice de transvection}
   \end{subfigure}\quad
   \begin{subfigure}{.3\textwidth}
      \centering
      \begin{tikzpicture}[line cap=round]
         \matrix[
            matrix of math nodes, ampersand replacement=\&,
            left delimiter=(, right delimiter=)
         ]{
            1 \& 0 \& 0\\ 
            0 \& 0 \& 1\\ 
            0 \& 1\& 0\\
         };
      \end{tikzpicture}
      \caption*{\color{DarkBlue1}Matrice de permutation}
   \end{subfigure}
\end{figure}

Enfin, on a \textbf{les matrices orthogonales} qui sont des matrices telles que leur inverse soit \textbf{leur transposée}, ce sont par exemple les matrices de rotation:
\begin{figure}[H]
   \color{DarkBlue1}
   \centering   
   \begin{subfigure}{.3\textwidth}
      \centering
      \begin{tikzpicture}[line cap=round]
         \matrix[
            matrix of math nodes, ampersand replacement=\&,
            left delimiter=(, right delimiter=)
         ]{
            \cos(\theta) \& -\sin(\theta)\\ 
            \sin(\theta) \& \cos(\theta) \\
         };
      \end{tikzpicture}
      \caption*{\color{DarkBlue1}Matrice de rotation}
   \end{subfigure}
\end{figure}
\subsection*{\subsecstyle{Lien avec les systêmes linéaires homogènes{:}}}
On peut aussi remarquer qu'il y a une correpondance directe et bijective entre les \textbf{systêmes linéaires homogènes} d'inconnues \( (x_1, \ldots, x_n) \) et les \textbf{équations vectorielles} d'inconnue \( X \), en effet on a:
\[ 
   \begin{cases}
      a_{1, 1}x_1 + \ldots + a_{1, p}x_p = 0\\
      \vdots\quad\quad\quad\quad\quad \vdots\quad\quad\quad\quad\quad \vdots\\
      a_{n, 1}x_1 + \ldots + a_{n, p}x_p = 0
   \end{cases} \iff f(x_1, \ldots, x_p) = (0, \ldots, 0) \iff AX = 0
\]
On peut alors montrer que l'ensemble des solutions est exactement le noyau de \( f \), de manière générale, le cas le plus courant est celui de \( n = p \) et la matrice est carrée, mais on a deux autres cas:
\begin{itemize}
   \item Si \( n > p \), on dira que le système est \textbf{sur-contraint} et généralement les solutions sont triviales.
   \item Si \( n < p \), on dira que le système est \textbf{sous-contraint} et généralement les solutions sont non-triviales.
\end{itemize}
Si on ajoute un second membre non-nul \( b \), alors le systême est dit \textbf{non-homogène}, dans ce cas si il existe une \textbf{solution particulière}, alors l'ensemble des solutions est un sous-espace affine\footnote[2]{Plus précisément, on a \( \mathscr{S} = \text{Ker}(f) + S_p \) où \( S_p \) est une solution particulière.} de direction le noyau de \( f \). Sinon l'ensemble des solutions est vide. 
\pagebreak
\subsection*{\subsecstyle{Opérations élémentaires{:}}}
Etant donnée une matrice \( M \in \mathscr{M}_n( \K) \), on définit une \textbf{opérations élémentaire sur les colonnes}\footnote[1]{Il peut être utile de noter que fondamentalement, on peut à la fois opérer \textbf{sur les colonnes} ou sur \textbf{les lignes}, en multipliant les matrices élémentaires par la gauche plutôt que par la droite.} de \( M \) par la multiplication à droite par une matrice élémentaire comme définies plus haut.\<

Ces opérations correpondent alors au fait d'échanger, de multiplier par un scalaire, ou de faire une combinaison linéaires des colonnes de la matrice. Ces opérations sont fondamentales car elles conservent beaucoup de propriétés:
\begin{itemize}
   \item Le noyau et sa dimension.
   \item L'image et sa dimension.
\end{itemize}
En particulier si \( \mathscr{E} \) est l'ensemble des matrices élémentaires on définit la relation d'équivalence:
\[ 
   M \sim M' \iff \exists E \in \mathscr{E} \; ; \; ME = M' 
\]
Informellement et sauf cas particuliers, on effectue souvent les opérations élémentaires directement sur la matrice sans expliciter la matrice élémentaire associée.
\subsection*{\subsecstyle{Echelonnements \& Calculs{:}}}
\addcontentsline{toc}{subsection}{Echelonnements et calculs}

La théorie permettant de lier matrices, applications linéaires et familles de vecteurs, l'étude du noyau et de l'image d'une matrice revêt alors une importantce capitale qui est l'objet de cette partie.\<

On dira qu'une matrice \(M\) est \textbf{échelonée} si la matrice obtenue aprés application d'opérations élémentaires est de la forme générale:
\begin{center}
   \begin{tikzpicture}[    
      every left delimiter/.style={xshift=.55em},
      every right delimiter/.style={xshift=-.55em}
   ]
      \matrix[
         matrix of math nodes, ampersand replacement=\&,
         left delimiter=(, right delimiter=),  outer sep = 0pt,inner sep=4.5pt
      ] (A) {
         {*} \& {0} \& {0} \& {0}\& {0}\\ 
         {*} \& {*} \& {0} \& {0}\& {0}\\ 
         {*} \& {*} \& {0} \& {0}\& {0}\\ 
         {*} \& {*} \& {*} \& {0}\& {0}\\
      };
      \draw[color=BrightBlue1, line width=0.5mm] (A-1-1.north west) -- (A-1-1.north east) -- (A-1-1.south east) -- (-0.25, 0.54)-- (-0.25, -0.64) -- (0.25, -0.64)-- (A-4-3.south east);
   \end{tikzpicture}   
\end{center}
En d'autres termes, on \textbf{creuse} la matrice pour faire apparaître des zéros sur la partie triangulaire supérieure.\+
Il existe aussi une variante appellé \textbf{échelonnement avec mémorisation}, pour cette variante, on nomme chaque vecteur-colonne de la matrice et on reporte toutes les transformations réalisées sur ces vecteurs. \<

Présentons maintenant les différentes informations que nous pouvons tirer de ces échellonements:

\begin{enumerate}
   \item On peut trés facilement trouver le rang d'une matrice, en effet aprés échelonnement, c'est simplement \textbf{le nombre de colonnes non-nulles} de la matrice. En particulier, il est donc trés facile de montrer qu'une matrice est inversible ou qu'une famille est une base.
   \item On peut trouver une base d'un sous-espace engendré par une famille, pour cela on échelonne la matrice de cette famille, l'ensemble des colonnes non nulles consitue une base du sous-espace initial.
   \item On peut trouver un supplémentaire d'une famille, on échelonne la matrice de la famille, et on complète la matrice par des vecteurs de la base canonique\footnote[2]{Ou alors de telle sorte que la matrice reste échelonnée.}, ces vecteurs formeront alors un supplémentaire.
   \item On peut trouver les équations cartésiennes d'un sous-espace engendré par une famille, pour cela on augmente la matrice de la famille par une matrice d'indéterminées, et on échelonne. Alors la dernière colonne fournira les equations cartésiennes du sous-espace.
   \item On peut trouver le \textbf{noyau, l'image} d'une matrice, gràce à la variante \textbf{avec mémorisation}, alors les colonnes nulles aprés échelonnement permettent d'en déduire des vecteurs envoyés du noyau, et les colonnes non-nulles constituent une base de l'image (voir l'exemple ci-aprés).
   \item On peut résoudre les sytèmes de la forme \( AX = B \) et donc inverser des matrices par une extension de cette méthode.
\end{enumerate}
\subsection*{\subsecstyle{Exemples d'utilisations{:}}}
Soit \(M = (C_1, C_2, C_3) = \begin{pmatrix} 1 & 2 & 1\\ 3 & 4 & 2\\ 5 & 6 & 3\end{pmatrix}\), nous allons présenter quelques exemples:
\begin{itemize}
   \item Cherchons le rang de la matrice, on a:
   \[
      M = \begin{pmatrix} 1 & 2 & 1\\ 3 & 4 & 2\\ 5 & 6 & 3\end{pmatrix} \sim  \begin{pmatrix} 1 & 0 & 0\\ 3 & 2 & 2\\ 5 & 4 & 4\end{pmatrix} \sim  \begin{pmatrix} 1 & 0 & 0\\ 3 & 2 & 0\\ 5 & 4 & 0\end{pmatrix} \sim  \begin{pmatrix} 1 & 0\\ 3 & 2\\ 5 & 4\end{pmatrix}
   \]
   La matrice est échelonnée sur \(2\) colonnes, donc son rang est \(2\). En particulier, \((C_1, C_2, C_3)\) n'est pas une base et la matrice (et l'endomorphisme associé) ne sont pas inversibles.
   \item Pour trouver un sous-espace supplémentaire à \((C_1, C_2, C_3)\) en reprenant l'échelonnement ci-dessus, on a simplement à rajouter des colonnes à \(M\) de sorte qu'elle soit échelonnée sur 3 colonnes, par exemple:
   \[
      M' = \begin{pmatrix} 1 & 0 & 0\\ 3 & 2 & 0\\ 5 & 4 & 1\end{pmatrix}
   \]
   Et donc le sous-espace engendré par le vecteur \((0, 0, 1)\) est bien un supplémentaire de l'espace des colonnes.
   \item Cherchons des équations cartésiennes du sous-espace des colonnes, on a:
   \[
      M = \begin{pmatrix} 1 & 2 & 1 & x\\ 3 & 4 & 2 & y\\ 5 & 6 & 3 &z\end{pmatrix} \sim  \begin{pmatrix} 1 & 0 & 0 & 0\\ 3 & 2 & 2 & y-3x\\ 5 & 4 & 4 & z-5x\end{pmatrix} \sim  \begin{pmatrix} 1 & 0 & 0 & 0\\ 3 & 2 & 0 & 0\\ 5 & 4 & x - 2y + z & 0\end{pmatrix}\sim  \begin{pmatrix} 1 & 0 & 0\\ 3 & 2 & 0\\ 5 & 4 & x - 2y + z\end{pmatrix}
   \]
   Alors l'équation cartésienne du sous-espace des colonnes est \(x - 2y + z = 0\).
   \item Cherchons le noyau et l'image de l'endomorphisme représenté par \(M\), on doit utiliser la mémorisation, ie:
   \[
      M = \begin{blockarray}{ccc} C_1 & C_2 & C_3 \\\begin{block}{(ccc)} 1 & 2 & 1\\3 & 4 & 2\\5 & 6 & 3\\\end{block}\end{blockarray} 
      \sim
      \begin{blockarray}{ccc} C_1 & C_2-2C_1 & 2C_3 - 2C_1 \\\begin{block}{(ccc)} 1 & 0 & 0\\3 & 2 & 2\\5 & 4 & 4\\\end{block}\end{blockarray} 
      \sim
      \begin{blockarray}{ccc} C_1 & C_2-2C_1 & 2C_3 - C_2 \\\begin{block}{(ccc)} 1 & 0 & 0\\3 & 2 & 0\\5 & 4 & 0\\\end{block}\end{blockarray} 
   \]
   On en conclut qu'une base de l'image est \([(1, 3, 5), (0, 2, 4)]\).\+
   Le noyau est de dimension 1 et par la mémorisation, une combinaison linéaire nulle est \(0C_1-C_2+2C_3\), le vecteur \((0, -1, 2)\) est bien dans le noyau et est donc une base du noyau.
   \item Cherchons la solution du système \( AX = (1, 2, 3) \), on construit alors la \textbf{matrice augmentée} ci-dessous qu'on échelonne en reportant les opérations sur la colonne augmentée:
   \begin{center}
      \begin{tikzpicture}[    
         every left delimiter/.style={xshift=.55em},
         every right delimiter/.style={xshift=-.55em}
      ]
         \matrix[
            matrix of math nodes, ampersand replacement=\&,
            left delimiter=(, right delimiter=),  outer sep = 0pt
         ] (A) {
            1 \& 2 \& 1 \& 1\\ 
            3 \& 4 \& 2 \& 2\\ 
            5 \& 6 \& 3 \& 3\\ 
         };
         \draw node at (1.5, 0) {$\sim$};
         \matrix[
            matrix of math nodes, ampersand replacement=\&,
            left delimiter=(, right delimiter=),  outer sep = 0pt
         ] (B) at (3, 0) {
            1 \& 2 \& 1 \& 1\\ 
            0 \& 2 \& 1 \& 1\\ 
            0 \& 0 \& 0 \& 0\\ 
         };
         \draw[color=black] (A-1-3.north east) -- (A-3-3.south east);
         \draw[color=black] (B-1-3.north east) -- (B-3-3.south east);
      \end{tikzpicture}   
   \end{center}
   On en conclut en repassant au systême que \( z \) est un paramètre libre, ie le système équivalent est:
   \[ 
      \begin{cases}
         x + 2y + z = 1\\
         2y + z = 2\\
         z \in \R 
      \end{cases} 
   \] 
   En résolvant pour \( (x, y) \), on trouve:
   \[ 
      \mathscr{S} = \left\{ (x, y, z) \in \R^3 \; ; \; (x, y, z) = \left(0, \frac{(1 + \lambda)}{2}, \lambda \right) , \lambda \in \R\right\} = \left(0, \frac{1}{2}, 0 \right) + \text{Ker}(f)
   \]
   Cette méthode permet aussi d'inverser une matrice inversible en faisant apparaître un produit matriciel dans le membre augmenté et l'identité dans le membre de gauche.
\end{itemize}
\chapter*{\chapterstyle{III --- Dualité}}
\addcontentsline{toc}{section}{Dualité}
On définit dans ce chapitre une notion fondamentale en algèbre linéaire, trés liée à celle de produit scalaire, qui est celle \textbf{d'espace dual} d'un espace vectoriel \(E\), qu'on notera \(E^*\) et qu'on définit par:
\begin{center}
   \textbf{L'espace dual d'un espace vectoriel est l'ensemble des formes linéaires sur cet espace.}
\end{center}
Attention, on parle ici de dual \textbf{algébrique}, on peut aussi définir un dual \textbf{topologique} en requierant que les formes linéaires considérées soit continues. Dans la plupart des exemples, on considèrera \(E = \R^2\) et donc par exemple un élément de \(E^*\) est:
\[
   \phi : (x, y) \mapsto 2x + y   
\]
Dans toute la suite, on se placera dans un espace \(E\) de dimension finie\footnote[1]{Tout ce qui suit est en général faux en dimension infinie, l'idée prinicipale étant qu'en dimension infinie, des considérations topologiques sont \textbf{nécéssaires}.}

\subsection*{\subsecstyle{Notations{:}}}
On introduit de nouvelles notations pratique, tout d'abord on utilisera souvent la notation \textbf{delta de Kronecker} qui pour tout \(n, m \in \N\) donne:
\[
   \begin{cases}
      \delta_{n}^m = 1 \text{ si } n = m\\
      \delta_{n}^m = 0 \text{ sinon }
   \end{cases}
\]
\subsection*{\subsecstyle{Propriétés{:}}}
On souhaite caractériser la forme d'une forme linéaire \(\phi\) de l'espace dual. On a directement par linéarité que:
\[
   \forall x \in E \; ; \; \phi(x) = \phi\left(\sum_{i}x^ie_i\right) = \sum_{i}x^i\phi(e_i)  
\]
Donc en particulier, on a que évidemment que \(\phi\) est caractérisée par ses images des vecteurs de base, et réciproquement si une application \(\phi'\) est telle que \(\phi = \sum_{i}x_ia_i\), alors \(\phi'\) est une forme linéaire sur \(E\).
\subsection*{\subsecstyle{Base duale{:}}}
Fixons une base \(\mathscr{B} = (e_1, \ldots, e_n)\) de \(E\), alors si on considère la famille de formes linéaires suivantes:
\[
   \begin{aligned}
      e_i^* : E &\longrightarrow \R \\
      \sum_{k \leq n}x_ke_k &\longmapsto x_i
   \end{aligned}
\]
Ce sont les projections sur les différentes coordonées. Alors cette famille forme une base de \(E^*\) qu'on appelera \textbf{base duale} de \(\mathscr{B}\) et qu'on notera \(\mathscr{B}^*\). Elle vérifie en outre la relation suivante:
\[
   e_i^*(e_j) = \delta_{i}^j
\]
\subsection*{\subsecstyle{Isomorphisme{:}}}
En particulier, si on fixe une base \( \mathscr{B} \) de \( E \), on a l'isomorphisme fondamental suivant:
\[ 
   \begin{aligned}
      \phi_{\mathscr{B}} : E &\longrightarrow E^* \\
      e_i &\longmapsto e_i^*
   \end{aligned} 
\]
C'est un isomorphisme non canonique car il dépends de la base choisie.
\subsection*{\subsecstyle{Base antéduale{:}}}
On considère alors le problème inverse, et on se donne une base \(\mathscr{B} = (\phi_1, \ldots, \phi_n)\) du dual de \(E\), alors il existe une unique base \((e_1, \ldots, e_n)\) de \(E\) telle que:
\[
   \forall i \in \inticc{1}{n} \; ; \; \phi_i = e_i^*   
\] 
Ou dit autrement il existe une base \(\mathscr{C}\) de \(E\) telle que \(\mathscr{C}^* = \mathscr{B}\), on dira alors que cette base est \textbf{l'antéduale} de \( \mathscr{B} \).
\subsection*{\subsecstyle{Hyperplans{:}}}
On rapelle alors qu'un hyperplan est par définition un sous-espace dont le supplémentaire est une droite, on peut alors caractériser les hyperplans en termes de formes linéaires par la proposition suivante:
\begin{center}
   \textbf{Les hyperplans sont exactements les noyaux de formes linéaires.}
\end{center}
\subsection*{\subsecstyle{Vecteurs contravariants{:}}}
Soit deux bases \( \mathscr{C}, \mathscr{B}\) de \(E\) et un vecteur \(u\) de coordonées respectives \(U, U'\), on a la propriété de changement de base suivante:
\[
   U' = \text{Pass}(\mathscr{C}, \mathscr{B})U
\]
On voit alors ici que les coordonées aprés le changement de base dépendent de \textbf{l'inverse de la matrice de passage}. On dira dans ce cas qu'un vecteur est \textbf{contravariant}\footnote[1]{Par rapport aux bases de l'espace de référence \( E \)}.
\subsection*{\subsecstyle{Vecteurs covariants{:}}}
Maintenant soit deux bases \( \mathscr{C}^*, \mathscr{B}^*\) de \(E^*\) et un covecteur \(u\) de coordonées respectives \(U, U'\), on peut utiliser la même formule pour montrer qu'on a:
\[
   U' = \text{Pass}(\mathscr{B}^*, \mathscr{C}^*)U
\]
Or, on peut alors montrer la propriété suivante fondamentale suivante:
\[
   \text{Pass}(\mathscr{B}^*, \mathscr{C}^*) = {}^tP
\]
En particulier on se ramène alors à une matrice de passage entre les bases de \(E\) et on a alors la formule de changement de base suivantes (pour les coordonées placées en ligne):
\[
   {}^tU' = {}^tUP
\]
On voit alors ici que les coordonées aprés le changement de base (en ligne) dépendent de \textbf{la matrice de passage}. On dira dans ce cas qu'un vecteur est \textbf{covariant par rapport aux bases de \(E\)}. Finalement moralement on a les formules de changement de bases suivantes:

\begin{itemize}
   \item Si \(X, Y\) sont des vecteurs : \(Y = P^{-1}X\)
   \item Si \(X, Y\) sont des covecteurs : \({}^tY = {}^tXP\)
\end{itemize}
\begin{center}
   \textbf{La notion de vecteurs contravariants et covariants est centrale en algèbre multilinéaire, elle permet la définition d'objets généraux appelés tenseurs qui généralisent ce concept.}
\end{center}
\chapter*{\chapterstyle{III --- Déterminant}} % Fini 99%
\addcontentsline{toc}{section}{Déterminant}

Soit \(A = (a_{i,j})\) une matrice carrée de taille \(n\), nous cherchons à définir une application \(\phi: \mathcal{M}_n(\K) \longrightarrow \R\) telle que:
\[
   A \in \text{GL}_n(\K) \Longleftrightarrow \phi(A) \neq 0
\]
\subsection*{\subsecstyle{Définition{:}}}
\addcontentsline{toc}{subsection}{Définition}
On définit l'application suivante appelée \textbf{déterminant}, qu'on note \(|A|\) ou \( \text{det}( \cdot) \) et qu'on définit par récurrence:
\begin{itemize}
   \item Si \(n = 1\) alors \(|(a)| = a \) 
   \item Sinon \(|A| := a_{1, 1}|A_{1, 1}| - a_{1, 2}|A_{1, 2}| + \ldots + (-1)^{n+1}a_{1, n}|A_{1, n}|\)
\end{itemize}
Où la notation \(A_{1, 1}\) signifie la matrice \(A\) à laquelle on a retiré la première ligne et la première colonne. \<

Cette opération s'appelle aussi \textbf{développement selon la première ligne} du déterminant, elle se comprends visuellement par:
\[
   |A| = \begin{vmatrix} a & b & c \\ d & e & f \\ g & h & i \end{vmatrix} = 
   a \begin{vmatrix} \square & \square & \square \\ \square & e & f \\ \square & h & i \end{vmatrix} - 
   b \begin{vmatrix} \square & \square & \square \\ d & \square & f \\ g & \square & i \end{vmatrix} + 
   c \begin{vmatrix} \square & \square & \square \\ d & e & \square \\ g & h & \square \end{vmatrix}   
\]
On régresse ainsi vers des déterminants de plus petite taille, et aprés plusieurs étapes, vers un réel.
\subsection*{\subsecstyle{Propriétés{:}}}
\addcontentsline{toc}{subsection}{Propriétés}
On peut montrer que cette application est \textbf{une forme multilinéaire alternée} en les colonnes de la matrice, en particulier, on a les propriétés suivantes:
\begin{itemize}
   \item Ajouter à une colonne une combinaison linéaire \textbf{des autres colonnes} ne change pas le déterminant.
   \item Echanger deux colonnes d'une matrice change le signe du determinant.
   \item Si deux colonnes sont égales ou qu'une des colonnes est nulle, le déterminant est nul.
\end{itemize}
\subsection*{\subsecstyle{Forme analytique{:}}}
\addcontentsline{toc}{subsection}{Forme analytique}
Le déterminant étant une application définie sur \( E^n \), on peut calculer son expression par rapport à une base \( (e_i)_{ i \leq n} \), en effet on a:
\[ 
   \text{det}(a_1, \ldots, a_n) = \text{det}\left( \sum_{i_1} a_{i_1, 1}e_{i_1}, \ldots, \sum_{i_n} a_{i_n, 1}e_{i_n} \right) = \sum_{i_1, \ldots, i_n \leq n}  a_{i_1, 1} \ldots a_{i_n, n} \times \text{det}(e_{i_1}, \ldots, e_{i_n})
\]

Mais beaucoup de ces termes sont nuls, en particulier les termes non nuls sont exactement les permutations des colonnes de la matrice, et donc on peut simplifier l'expression en:
\[ 
   \text{det}(a_1, \ldots, a_n) = \sum_{\sigma \in \mathfrak{S}_n} a_{ \sigma(1), 1} \ldots a_{\sigma(n) , n}\times \text{det}(e_{\sigma(1)}, \ldots, e_{\sigma(n)})
\]
Puis finalement, on remarque que le déterminant du membre de droite peut être remis dans le bon ordre modulo la signature de la permutation considérée et on obtient l'expression finale:
\[ 
   \text{det}(a_1, \ldots, a_n) = \sum_{\sigma \in \mathfrak{S}_n} \epsilon(\sigma) a_{ \sigma(1), 1} \ldots a_{\sigma(n) , n}
\]
Cette forme nous permet alors de montrer que le déterminant de la transposée est égale au déterminant de \( A \), et donc par la suite qu'on peut dévelloper un déterminant selon n'importe quelle ligne ou colonne et obtenir le même résultat au signe près.
\subsection*{\subsecstyle{Propriété de morphisme{:}}}
Muni de cette forme explicite, on peut alors montrer que le déterminant est un \textbf{multiplicatif}, ie on a que:
\[ 
   \forall A, B \in \mathscr{M}_n(\K) \; ; \; \text{det}(AB) = \text{det}(A)\text{det}(B) 
\]
En particulier si \( A \) est inversible, on montre alors que \( \text{det}(A^{-1}) = (\text{det}(A))^{-1} \) et donc que \( \text{det}(A) \) n'est pas nul\footnote[1]{La réciproque, importante, de ce résultat sera montrée dans la section sur la comatrice et permettra de conclura que le déterminant répond bien à la question introductive de ce chapitre.}. De plus on montre de la même manière que le déterminant est un \textbf{invariant de similitude}.
\subsection*{\subsecstyle{Déterminant d'une famille{:}}}
Si on considère une famille \(\Fam = (e_1 , \ldots, e_n)\) de vecteurs de coordonées \((C_1, \ldots, C_n)\) dans une certaine base \(\mathscr{B}\), alors on peut définir le déterminant \textbf{d'une famille de vecteurs} dans cette base par:
\[
   \text{det}_{\mathscr{B}}(\Fam) = \text{det}_{\mathscr{B}}(C_1, \ldots, C_n)
\]
Soit deux bases \(\mathscr{B, B'}\), on pourrait alors considèrer \textbf{l'effet d'un changement de base} sur un tel déterminant et on a alors:
\[ 
   \text{Mat}_{\mathscr{B}'}(\Fam) = \text{Pass}(\mathscr{B}', \mathscr{B})\text{Mat}_{\mathscr{B}}(\Fam)
\]
Et donc le determinant dans la nouvelle base est égal à celui dans l'ancienne modulo le déterminant de la matrice de changement de coordonées.
\subsection*{\subsecstyle{Orientation d'une base{:}}}
On peut alors définir \textbf{l'orientation d'un espace vectoriel}, en effet, on dira que deux bases \(\mathscr{B}, \mathscr{B'}\) ont même orientation si et seulement si:
\[
   \text{det}(\text{Pass}(\mathscr{B}, \mathscr{B'})) > 0
\]
Si on fixe alors une base orientée canoniquement, on qualifie son orientation (et celles de toutes les bases de meme orientation) de \textbf{directe} et les bases d'orientation opposée sont alors d'orientation \textbf{indirecte}.Par exemple dans le cas de \(\R^n\), la base canonique est la base qui, par convention, est d'orientation directe.
\subsection*{\subsecstyle{Cofacteurs{:}}}
\addcontentsline{toc}{subsection}{Cofacteurs}

Reprenons en simplifant la définition donnée plus haut du développement selon la i-ème ligne:
\[
   |A| := a_{i, 1}|A_{i, 1}| - a_{i, 2}|A_{1, 2}| + \ldots + (-1)^{n+i}a_{i, n}|A_{i, n}| = \sum_{j=1}^{n}a_{i, j}(-1)^{i+j}|A_{i, j}|
\]
On appelle alors \textbf{cofacteur} de l'élément \(a_{i, j}\) le scalaire:
\[ 
   \text{cof}(a_{i, j}) := (-1)^{j+i}|A_{i, j}|
\]
Ce sont simplement les déterminants mineurs que l'on calcule à chaque itération \textbf{en tenant compte du signe qui précède le coefficient}. En pratique pour trouver le signe du cofacteur, on ne calcule pas \((-1)^{i + j}\), mais on le détermine par la règle de l'échiquier qu'on peut se représenter comme suit:
\[
   \begin{vmatrix} + & - & + & - & \ldots \\ - & + & - & + &\ldots \\ + & - & + & - &\ldots \\ \vdots & \vdots & \vdots & \vdots \\\end{vmatrix}
\]

\underline{Exemple:} Pour \(\begin{pmatrix}
   1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9
\end{pmatrix}\), le cofacteur du coefficient en position \((1, 2)\) est 
\(- \begin{vmatrix} 4 & 6 \\ 7 & 9\end{vmatrix} \)
\pagebreak
\subsection*{\subsecstyle{Comatrice{:}}}
\addcontentsline{toc}{subsection}{Comatrice}

On peut alors définir \textbf{la comatrice} d'une matrice \(A\) donnée, c'est en fait simplement \textbf{la matrice des cofacteurs} de \(A\), ie le terme en position \((i, j)\) de la comatrice est exactement le cofacteur de l'élement en position \((i, j)\). \<

L'intérêt principal de la comatrice est de calculer l'inverse de matrices inversibles, en effet, on peut montrer l'identité:
\[
   A \times \text{com}(A)^{\top} = \text{det}(A)I_n   
\]
Et en particulier si le déterminant est non-nul, alors \( A \) est \textbf{inversible} et on a une formule explicite pour l'inverse de \( A \) donnée par:
\[ 
   A^{-1} = \frac{1}{\text{det}(A)}\text{com}(A)^{\top}  
\]
Cette formule est néanmoins particulièrement inexploitable pour \(n > 3\) du fait de la quantité de calcul à réaliser.
\subsection*{\subsecstyle{Conclusion du problème introductif{:}}}
\begin{itemize}
   \item Dans la section sur la propriété de morphisme on a montré que si \( A \) est inversible, alors nécéssairement son déterminant est non-nul. 
   \item Dans la section sur la comatrice, on a montré que réciproquement si le déterminant de la matrice est non-nul, alors elle est inversible.
\end{itemize}
On a donc bien montré que l'application déterminant ainsi définie répond bien au problème posé en début de chapitre et caractérise bien l'inversibilité d'une matrice. En outre c'est un morphisme de groupe: 
\[ 
   \text{det}( \cdot) : GL_n( \K) \longrightarrow \K^*
\]
Et en outre il ne dépends pas de la représentation matricielle choisie vu que c'est un invariant de similitude. C'est donc une quantité intrinséque des endomorphismes.
\subsection*{\subsecstyle{Volume orienté{:}}}
\addcontentsline{toc}{subsection}{Volume orienté}

Le déterminant admet un interprétation géométrique intéressante liée au concept d'aire, de volumes, et d'hypervolumes de manière générale. \<

Considérons le cas simple d'une base \(\mathscr{B} = (e_1, e_2)\) de vecteurs de \(\R^2\), alors le déterminant de cette base correspond \textbf{à l'aire algébrique\footnote[1]{C'est une aire ``signée'', ie l'aire géométrique est donc la valeur absolue de cette aire algébrique.} du parallélogramme formé par les deux vecteurs}, ie:
\begin{center}
   \begin{tikzpicture}[xscale=1.6, yscale=1.4]
      \draw[black!15] (-0.5,0) -- (4,0);
      \draw[black!15] (0, -0.5) -- (0,2);

      \draw[color=black!0, fill=BrightBlue1!25] (0,0) -- (0.6, 1.5) -- (3.1, 1.9) -- (2.5, 0.4);

      \draw node[color=DarkBlue1, rotate=8] at (1.55, 0.95) {$|\text{det}(e_1, e_2)|$};

      \draw[-latex, color=DarkBlue1, thick] (0, 0) -- (0.6, 1.5) node[left] {$e_1$};
      \draw[-latex, color=DarkBlue1, thick] (0, 0) -- (2.5, 0.4) node[below right] {$e_2$};

      \draw[color=DarkBlue1, dashed] (0.6, 1.5) -- (3.1, 1.9)-- (2.5, 0.4);


   \end{tikzpicture}
\end{center}
Plus généralement, le déterminant d'une famille de \(n\) vecteurs est \textbf{l'hypervolume algébrique} du parallélotope formé par ces \(n\) vecteurs.

\chapter*{\chapterstyle{III --- Introduction à la réduction}}
\addcontentsline{toc}{section}{Introduction à la réduction}
Dans ce chapitre, nous étudirons un domaine vaste de l'algèbre linéaire appellé \textbf{réduction des endormorphismes}.\+
En effet, sous une forme quelconque un endomorphisme représenté par une matrice présente plusieurs problèmes:
\begin{align*}
   &\bullet \;\; \text{Il est couteux de calculer les puissances d'une matrice quelconque.} \\
   &\bullet \;\; \text{Une représentation quelconque donne peu d'informations sur l'endomorphisme.}
\end{align*}
L'objectif sera donc de réduire (comprendre simplifier) la représentation de l'endormorphisme, et de le représenter par une matrice plus simple. \<

Soit \(f \in \mathcal{L}(E)\) et une famille \((E_i)\) de sous-espaces \textbf{supplémentaires}\footnote[1]{Comme tout sous-espace admet un supplémentaire (théorème de la base incomplète), on peut définir une base adaptée à \textbf{un seul} sous-espace comme étant une base adaptée à la somme directe de ce sous-espace et de son supplémentaire, qui revient à compléter la base en une base de \(E\).}, alors on peut construire une base \(\mathcal{B}\) dite \textbf{base adaptée à la décomposition} en concaténant des bases respectives des \(E_i\).

\subsection*{\subsecstyle{Elements Propres {:}}}
Soit \(\lambda \in \R\), on dit que \(\lambda\) est une \textbf{valeur propre}\footnote[2]{On appelle \textbf{specte}, noté Sp\((f)\) l'ensemble des valeurs propres de \(f\).} de l'endomorphisme \(f\) si et seulement si il existe un vecteur non-nul \(u \in E\) tel que:
\customBox{width=2.5cm}{
   \(f(u) = \lambda u\)
}
On dira alors qu'un tel vecteur est \textbf{vecteur propre} de l'endomorphisme et on appelle \textbf{sous-espace propre} associé à la valeur propre \(\lambda\) l'ensemble des vecteurs propres associés, qu'on note \(E_\lambda\) dont on déduit une expression\footnote[3]{Directement d'après la définition d'un vecteur propre associé à \(\lambda\).}:
\customBox{width=5cm}{
   \(E_\lambda = \text{Ker}(f - \lambda \text{Id})\)
}
Enfin, on peut montrer une propriété trés importante pour la suite:
\customBox{width=10cm}{
   Toute somme de sous-espaces propres est \textbf{directe}.
}
\begin{center}
   \textit{L'image des sous-espaces propres par l'endomorphisme se réduit à une homotéthie de rapport la valeur propre.}
\end{center}

\subsection*{\subsecstyle{Sous-Espaces Stables {:}}}
On dit que \(F\) est \textbf{stable par l'endormorphisme} si et seulement si:
\customBox{width=2.5cm}{
   \(f(F) \subseteq F\)
}
On considère maintant une base de \(F\) qu'on compléte en une base de \(E\) via le théorème de la base incomplète, alors dans une telle base, l'endormorphisme est représenté par la matrice par blocs:
\[
   \left(\begin{array}{c|c}
      A & B\\
      \hline\\[-1.7\medskipamount]
      0 & C
   \end{array}\right)
\]
\pagebreak

Plus généralement, si on a une famille \((E_i)\) de sous-espaces \textbf{stables et supplémentaires}, ie \(E = \bigoplus_{i \in \N} E_i\), et \(\mathcal{B}\) est une base adaptée à cette décomposition, alors dans cette base, \(f\) est représentée par la matrice diagonale par blocs:
\[
   \left(\begin{array}{ccc}
      A_1 & {} & {}\\
      {} & \ddots & {}\\
      {} & {} & A_n\\
   \end{array}\right)
\]
On remarque donc que la stabilité des sous-espaces nous permet de reprénsenter notre transformation de manière plus simple, en particulier, on peut alors montrer une propriété fondamentale:
\customBox{width=10cm}{
   \textbf{Tout les sous-espaces propres sont stables.}
}

\subsection*{\subsecstyle{Polynôme caractéristique {:}}}
On peut montrer\footnote[1]{\(E_\lambda\) est un noyau, il suffit de caractériser le fait qu'il soit non vide en termes de la bijectivité d'un certain endomorphisme.} que \(\lambda\) est valeur propre si et seulement si:
\begin{align*}
   E_\lambda \neq \bigl\{0_E\bigl\} \Longleftrightarrow \text{det}(f - \lambda \text{Id}) = 0
\end{align*}
On définit alors le \textbf{polynôme caractéristique} d'un endormorphisme par:
\customBox{width=5cm}{
   \(P_f=\text{det}(f - X \text{Id})\)
}
En particulier, on peut donc montrer que:
\customBox{width=15cm}{
   \textbf{Les valeurs propres sont exactement les racines du polynôme caractéristique.}
}
Ceci nous donne donc une méthode systématique pour trouver les valeurs propres d'un endomorphisme. En particulier, on peut alors montrer les identités suivantes, utiles dans la recherche de valeurs propres:
\begin{align*}
   &\bullet \;\; \text{La somme des valeurs propres est égale à \textbf{la trace de la matrice}.} \\
   &\bullet \;\; \text{Le produit des valeurs propres est égale au \textbf{déterminant de la matrice}.}
\end{align*}

\subsection*{\subsecstyle{Diagonalisation {:}}}
Les endomorphismes qu'on peut représenter le plus simplement sont ceux qui ceux réduisent (dans une base bien choisie) à une homotéthie des vecteurs de la base. On dira alors que ces endomorphismes sont \textbf{diagonalisables}, formellement:
\customBox{width=16.5cm}{
   Un endormorphisme est diagonalisable si il existe une base de \(E\) constituée de vecteurs propres de \(f\).
}
De manière équivalente:
\customBox{width=16cm}{
   Un endormorphisme est diagonalisable si ses matrices sont semblables à une matrice diagonale.
}

\underline{Exemple:} Soit \(f\) un tel endormorphisme de \(\R^3\) et \((e_{\lambda_1}, e_{\lambda_2}, e_{\lambda_3})\) des tels vecteurs propres, alors dans cette base, \(f\) est représenté par la matrice:
\[
   D = \left(\begin{array}{ccc}
      \lambda_1 & {} & {}\\
      {} & \lambda_2 & {}\\
      {} & {} & \lambda_3
   \end{array}\right) 
\]
Ou encore si \(A\) est la matrice de \(f\) dans la base canonique, alors \(A = PDP^{-1}\) avec:
\[
   P = ([e_{\lambda_1}]_\mathscr{C}, [e_{\lambda_2}]_\mathscr{C}, [e_{\lambda_3}]_\mathscr{C}) \text{ et } D = \left(\begin{array}{ccc}
      \lambda_1 & {} & {}\\
      {} & \lambda_2 & {}\\
      {} & {} & \lambda_3
   \end{array}\right) 
\]
\begin{center}
   \textit{Diagonaliser un endomorphisme revient à \textbf{décomposer l'espace en somme directe de droites stables}.
   }
\end{center}
\pagebreak

\subsection*{\subsecstyle{Critères de diagonalisabilité {:}}}
On sait qu'un endomorphisme \(f\) est diagonalisable si et seulement si il admet une base de vecteurs propres, alors on peut montrer\footnote[1]{En effet, si tel est le cas, alors il suffit de prendre une base pour chaque \(E_\lambda\) (qui est bien constituée de vecteurs propres par définition), et de les concaténer pour obtenir une base de \(E\) constituée de vecteurs propres.} que \(f\) est diagonalisable si et seulement si:
\customBox{width=4cm}{
   \(
      E = \bigoplus_{\lambda \in \text{Sp}(f)} E_\lambda  
   \)
}
En particulier on remarque alors que montrer la supplémentarité revient à montrer que \textbf{la somme des dimensions des sous-espaces propres est égale à la dimension totale} car toute somme de sous-espaces propres est directe.\<

On peut alors montrer que si \(\lambda\) est une valeur propre de multiplicité \(\alpha\) pour le polynome caractéristique, alors:
\[
   1 \leq \text{dim}(E_\lambda) \leq \alpha   
\]
On a alors le théorème fondamental suivant:
\begin{center}
   \textbf{Un endomorphisme est diagonalisable si et seulement si son polynôme est scindé sur \(\K\) et que la dimension de chaque sous-espace propre est égale à la multiplicité de la valeur propre associée}.
\end{center}
On peut donc étudier si un endormorphisme est diagonalisable en calculant les valeurs propres et les dimensions des sous-espaces propres associés, ce qui revient à un calcul de polynôme caractéristique suivi de calculs de noyau.

\underline{Exemple:} Diagonalisons la matrice \(A = \left(\begin{array}{ccc}
   1 & 1 & 1\\
   2 & 2 & 2\\
   3 & 3 & 3
\end{array}\right) \)\< 

On peut calculer \(P_A = \text{det}(A - XI_3) = X^2(X-6)\), on a alors deux sous-espaces propres, \(E_0\) et \(E_6\) et on sait que \(E_0\) est de dimension \(1\), il suffit alors de vérifier que \(E_6\) est bien de dimension \(2\) pour conclure que \(\sum \dim(E_\lambda) = \dim(E)\) et donc que la matrice est diagonalisable.\<

Pour trouver une base de vecteurs propres, il suffit alors de trouver une base de \(E_0, E_6\) et de la concaténer en une base de \(E\).
\chapter*{\chapterstyle{III --- Polynomes d'endomorphismes}}
\addcontentsline{toc}{section}{Polynomes d'endomorphismes}
L'objet principal de ce chapitre est l'étude des polynômes d'endomorphismes et de matrices, en effet, les matrices est les endomorphismes formant une algèbre, on peut en calculer des puissances, des sommes, et effectuer un multiplication externe, on peut donc définir des \textbf{polynômes de matrices/d'endomorphismes}, en effet si on a \(P = \sum_{k=0}^{n} a_kX^k \in \K[X]\) et \(u\) un endomorphisme, on définit alors:
\[
   P(u) = \sum_{k=0}^{n}a_k u^k  
\]
\subsection*{\subsecstyle{Propriétés {:}}}
On définit alors un \textbf{morphisme d'algèbre} pour un endomorphisme donné par:
\[
   \begin{aligned}
      \phi_u:  \K[X] &\longrightarrow \mathcal{L}(E) \\
      P &\longmapsto P(u)
   \end{aligned}
\]
En particulier, on a donc \(PQ(u) = P(u) \circ Q(u)\), on peut alors en déduire la proposition suivante:
\begin{center}
   \textbf{Deux polynôme d'un même endomorphisme commutent.}
\end{center}
On peut alors montrer que les polynômes d'endomorphismes ont un bon comportement vis-à-vis
des changements de bases, en particulier si \(A = PBP^{-1}\), pour tout polynôme \(Q\), on montre facilement que:
\[
   Q(A) = PQ(B)P^{-1}   
\]
Enfin, on montre aussi que si \(u\) est représenté par \(A\) dans une base, alors \(u^k\) est représenté par \(A^k\) dans cette même base, et donc par linéarité \(P(u)\) est représenté par \(P(A)\) dans cette base. En particulier, le polynôme d'un endomorphisme ne dépends alors par de la représentation choisie.
\subsection*{\subsecstyle{Valeurs propres d'un polynôme d'endomorphisme {:}}}
Pour un endormorphisme \(u\) admettant une valeur propre \(\lambda\) de vecteur propre associé \(v\), on peut alors étudier le lien entre les polynômes d'endomorphisme et les valeurs propres, et en particulier, on peut montre que \(\lambda^k\) est valeur propre de \(u^k\) et donc par linéarité que:
\[
   P(u)(V) = P(\lambda)(V)
\]
Et donc que \(P(\lambda)\) est valeur propre de \(P(u)\).
\subsection*{\subsecstyle{Polynômes annulateurs {:}}}
On considère \(u \in \mathcal{L}(E)\), et on définit l'ensemble des \textbf{annulateurs} de \(u\) par:
\[
   \mathscr{A}_u := \Bigl\{ P \in \K[X] \; ; \; P(u) = 0_{\mathcal{L}(E)} \Bigl\}   
\]
Ce sont l'ensemble des polynômes qui annulent \(u\). On définit de même les polynômes annulateurs de matrices. Une propriété fondamentale est alors que cette ensemble n'est jamais vide\footnote[1]{Il suffit de considèrer la dimension de \(E\), et une famille plus grande que cette dimension, donc liée, et on peut alors trouver un polynôme en \(u\) qui s'annule.}, en effet on a que:
\begin{center}
   \textbf{Tout endormoprhisme admet un polynôme annulateur non-nul.}
\end{center}
On peut alors étudier le lien entre les valeurs propres d'un endomorphisme et ses annulateurs, et on peut alors montrer la propriété suivante:
\[
   \text{Sp}(u) \subseteq \bigl\{ \alpha \in \K \; ; \; P(\alpha) = 0 \bigl\}   
\]
\begin{center}
   \textit{Les seules valeurs propres possibles sont les racines de l'annulateur}.
\end{center}
\pagebreak
Néanmoins il n'y a pas équivalence, plus précisément, si \(A_u \in \mathscr{A}_u\), et si on définit \(Q_u = (X - \lambda_1)\ldots(X - \lambda_k)\) où les \((\lambda_i)\) sont toutes les valeurs propres de \(u\), alors on a:
\customBox{width=4cm}{
   \(
      Q_u \; |\;  A_u   
   \)   
}
\subsection*{\subsecstyle{Polynôme minimal {:}}}
On considère l'ensembe des annulateurs d'un endomorphisme \(u\), alors il est non-vide comme énoncé ci-dessus, et il admet aussi \textbf{un plus petit élément unitaire} (au sens du degré) et il est unique.\<

On appelle alors ce plus petit élément \textbf{le polynôme minimal} de \(u\) qu'on note \(M_u\)
\subsection*{\subsecstyle{Théorème de Cayley-Hamilton {:}}}
On peut alors énoncer le théorème fondamental de la réduction des endormorphismes, ie le \textbf{théorème de Cayley-Hamilton}:
\begin{center}
   \textbf{Le polynôme caractéristique est un annulateur.}
\end{center}
La démonstration, non-triviale, se fait par un argument topologique et par la continuité de la fonction polynôme caractéristique. On a donc la relation avec le polynôme minimal suivante:
\[
   M_u \; | \; P_u   
\]
\subsection*{\subsecstyle{Lemme des noyaux {:}}}
On s'intéresse finalement aux \textbf{noyaux de polynômes d'endomorphismes} pour pouvoir énoncer le dernier théorème de cette partie. On peut tout d'abord montrer facilement le résultat suivant:
\begin{center}
   \textbf{Le noyau d'un polynôme d'endomorphisme est stable par celui-ci.}
\end{center}
Soit \(P, Q\) deux polynômes \textbf{premiers entre eux}, on peut alors montrer\footnote[1]{La démonstration est non-trivial et fait appel à la relation de Bezout pour les polynômes.} le \textbf{lemme des noyaux}, c'est à dire que:
\customBox{width=7cm}{
   \(\Ker{PQ(u)} = \Ker{P(u)} \oplus \Ker{Q(u)}\)
}
En particulier, pour \(P\) un polynôme annulateur de \(u\) qui se décompose en \(P_1, \ldots, P_k\), on a la décomposition suivante de l'espace tout entier:
\[
   E = \bigoplus_{k=1}^n \Ker{P_k(u)}
\]
\subsection*{\subsecstyle{Caractérisations via les annulateurs {:}}}
On peut caractériser la diagonalisabilité via les annulateurs, en effet, on peut montre via le lemme des noyaux qu'on a:
\begin{center}
   \textbf{Un endomorphisme est diagonalisable si et seulement si il admet un annulateur scindé à racines simples}.
\end{center}
On peut aussi caractériser la trigonalisabilité par:
\begin{center}
   \textbf{Un endomorphisme est trigonalisabilité si et seulement si il admet un annulateur scindé}.
\end{center}
\chapter*{\chapterstyle{III --- Trigonalisation}}
\addcontentsline{toc}{section}{Trigonalisation}
Les endomorphismes qu'on ne peut représenter sous forme diagonale nous posent alors problème, on cherche alors dans ce chapitre à mobiliser la théorie des polynômes d'endomorphismes pour comprendre les conditions pour représenter de tels endomorphismes sous une forme plus simple triangulaire, ou sous \textbf{forme de Dunford} qui sera présentée ci-dessous.

\subsection*{\subsecstyle{Critère de trigonalisation {:}}}
On peut montrer le critère suivant:
\begin{center}
   \textbf{Un endomorphisme \(u\) est trigonalisable sur \(\K\) si et seulement si son polynôme caractéristique est scindé sur \(\K\).}
\end{center}
En particulier tout les endormorphismes sont trigonalisables dans \(\C\).\<

Néanmoins, on comprends vite qu'une forme triangulaire quelconque sera peu utile car on ne pourra calculer ses puissances facilement, on peut alors montrer que si \(u\) est trigonalisable, il admet une forme plus simple encore appellée \textbf{forme de Dunford}:
\[
   \begin{tikzpicture}[every left delimiter/.style={xshift=2mm},
         every right delimiter/.style={xshift=-2mm}]
      \path (0,0) node (mat) [matrix,matrix of math nodes,left delimiter=(,right delimiter=),column sep=2mm,row sep=1mm]
      {
      |(A)|\lambda_1 & * & * & 0 & 0 & 0\\
      0 & \lambda_1 & * & 0 & 0 & 0\\
      0 & 0 & |(B)|\lambda_1 & 0 & 0 & 0\\
      0 & 0 & 0 & |(C)|\lambda_2 & * & 0\\
      0 & 0 & 0 & 0 & |(D)|\lambda_2 & 0\\
      0 & 0 & 0 & 0 & 0 & |(E)|\ddots \\
      };
      \path 
      (B)--(C) coordinate[midway] (P)
      (D)--(E) coordinate[midway] (Q)
      ;
      \begin{scope}
         \draw[fill=BrightRed1!20] (P) rectangle (Q);
         \fill[BrightBlue1!20] (P) rectangle (A.north west);
         \draw 
         (P)--(P-|A.west) (P)--(P|-A.north);
      \end{scope}
   \end{tikzpicture}
\]
C'est une matrice \textbf{triangulaire par blocs triangulaires}. Et les puissances de telles matrices sont alors facile à calculer via le produit par blocs et le binôme de Newton. En effet chaque bloc et de la forme \(\lambda I_n + N\) avec \(N\) nilpotente, donc le binôme simplifie grandement les calculs.

\subsection*{\subsecstyle{Structure des noyaux itérés {:}}}
Soit \(u\) un endomorphisme, alors on peut montrer que les noyaux des puissances de \(u\) forment la structure suivante:
\[
   \Ker{u} \subsetneq \Ker{u^2} \subsetneq \ldots \subsetneq \Ker{u^k}   
\]
Et cette suite de noyaux itérés est \textbf{stationnaire}, en particulier, si \(u\) est nilpotent, elle est stationnaire et le dernier sous espace est \(E\) tout entier.
\subsection*{\subsecstyle{Sous-espaces caractéristiques {:}}}
Soit \(u\) un endomorphisme de polynôme caractéristique \(P_u = (X - \lambda_1)^{\alpha_1}\ldots(X - \lambda_k)^{\alpha_k}\), alors on appelle \textbf{sous-espace caractéristique} associé à la valeur propre \(\lambda_k\) le sous-espace suivant:
\[
   F_{\lambda_k} = \Ker{(u - \lambda_k\text{Id})^{\alpha_k}}   
\]
On sait que les \((X - \lambda_k)^{\alpha_k}\) sont premiers entre eux, donc d'aprés le théorème de Cayley-Hamilton et le lemme des noyaux, on a:
\[
   E = F_{\lambda_1} \oplus \ldots \oplus F_{\lambda_k}
\]
Et donc en particulier on a \(\dim(F_{\lambda_1}) = \alpha_1\).
\begin{center}
   \textit{Les sous-espaces caractéristiques sont des sous-espaces propres "sympathiques".}
\end{center}
C'est sont aussi des noyaux de polynômes d'endomorphismes donc en particulier, ils sont stables par \(u\). Par ailleurs, d'aprés la structure des noyaux itérés, on a:
\[
   E_{\lambda_k} = \Ker{(u - \lambda_k\text{Id})^1} \subsetneq \Ker{(u - \lambda_k\text{Id})^2} \subsetneq \ldots \subsetneq \Ker{(u - \lambda_k\text{Id})^{\alpha_k}} = F_{\lambda_k}
\]
Ce sont ces sous-espaces qui nous permettront de construire un base de \(E\) dans laquelle \(u\) est représenté par une matrice de Dunford.
\subsection*{\subsecstyle{Trigonalisation de Dunford {:}}}
On peut alors définir une méthode générale de trigonalisation de Dunford, on considère un endomorphisme \(u\) et son polynôme caractéristique, alors on obtient une base de trigonalisation de Dunford par l'algorithme suivant:
\begin{itemize}
   \item Si la dimension du sous-espace propre \(E_\lambda\) est égale à la multiplicité, le bloc associé à \(\lambda\) est diagonal, et la base recherchée est une base du sous-espace propre
   \item Sinon, on calcule une \textbf{base adaptée} aux noyaux itérés \(E_\lambda \subsetneq \Ker{(u - \lambda\text{Id})^2} \subsetneq \ldots \subsetneq F_\lambda\) via le théorème de la base incomplète puis les coordonées de l'image de cette base par \(u\) pour obtenir le bloc associé à \(\lambda\).
\end{itemize}

\underline{Exemple:} Dans toute la suite nous considérerons l'exemple de l'endomorphisme de \(\R^5\) de polynôme caractéristique \(P_u = (X - 2)^2(X - 3)^3\), alors d'après le lemme des noyaux et le théorème de Cayley-Hamilton, on a:
\[
   E = F_2 \oplus F_3 
\]
Les valeurs propres sont \(2, 3\) et on supposera que la première valeur propre est telle que la dimension du sous-espace propre est égale à la multiplicité, alors on trouve aisément une base \(e_1, e_2\) du bloc associé à \(2\), il sera diagonal.\< 

Il nous suffit alors de trouver une base de \(F_3 = \Ker{(u - 3\text{Id})^3}\) qu'on va calculer de la manière suivante:
\begin{itemize}
   \item On calcule une base de \(E_3 = \Ker{u - 3\text{Id}}\)
   \item On la complète en une base de \(\Ker{(u - 3\text{Id})^2}\)
   \item On la complète en une base de \(F_3 = \Ker{(u - 3\text{Id})^3}\)
\end{itemize}
Finalement, on a \(F_3\) de dimension \(3\) et donc une base \(e_3, e_4, e_5\) de \(F_3\). La base finale recherchée est donc \((e_1, e_2, e_3, e_4, e_5)\) et dans cette base la matrice est de la forme:
\[
   \begin{tikzpicture}[every left delimiter/.style={xshift=2mm},
         every right delimiter/.style={xshift=-2mm}]
      \path (0,0) node (mat) [matrix,matrix of math nodes,left delimiter=(,right delimiter=),column sep=2mm,row sep=1mm]
      {
      2 & 0 & 0 & 0 & 0\\
      0 & 2 & 0 & 0 & 0\\
      0 & 0 & 3 & * & *\\
      0 & 0 & 0 & 3 & *\\
      0 & 0 & 0 & 0 & 3\\
      };
   \end{tikzpicture}
\]
Où les coefficients \(*\) sont donnés par le calcul des cordonnées des images des vecteurs dans la base. 
\chapter*{\chapterstyle{III --- Applications de la réduction}}
\addcontentsline{toc}{section}{Applications de la réduction}
Dans cette dernière partie, on va maintenant pouvoir développer les applications possibles de la réduction dans la résolution de problèmes variés. On considère ici le cas d'une matrice 
\[
   A = \begin{pmatrix}
      2 & 1 & 1 \\
      1 & 2 & 1 \\
      0 & 0 & 3
   \end{pmatrix} = P \begin{pmatrix}
      1 & 0 & 0 \\
      0 & 3 & * \\
      0 & 0 & 3
   \end{pmatrix} P^{-1}
\]
Dans le cas plus simple de matrice diagonalisable, tout les calculs sont plus simples et les mêmes méthodes s'appliquent.

\subsection*{\subsecstyle{Calculs de puissances {:}}}
La première étape pour calculer une puissance de matrice via la réduction est de remarquer que:
\[
   A^k = (PTP^{-1})^k = PT^kP^{-1}   
\]
Or, \(T^k\) se calcule alors par blocs et on a:
\[
   T^k = \begin{pmatrix}
      B_1^k & \\
      & B_2^k \\
   \end{pmatrix} = \begin{pmatrix}
      1 & \\
      & B_2^k \\
   \end{pmatrix}
\]
Et on a alors \(B_2 = 3\text{Id} + N\) avec \(N\) strictement triangulaire donc nilpotente, et donc on calcule facilement sa puissance via le binôme de Newton car Id commute toujours.

\subsection*{\subsecstyle{Suites récurrentes {:}}}
Soit \(3\) suites \(u_n, v_n, w_n\) telles que \(u_1 = 1, v_1 = 1, w_1 = 1\)on considère maintenant le \textbf{système de suites récurrentes} suivant:
\[
   \begin{cases}
      u_{n} = 2u_{n - 1} + v_{n - 1} + w_{n - 1}\\
      v_{n} = u_{n - 1} + 2v_{n - 1} + w_{n - 1}\\
      w_{n} = 3w_{n - 1}\\
   \end{cases}   
\]
On pose alors \(U_n = \begin{pmatrix}
   u_n \\ v_n \\ w_n
\end{pmatrix}\) et le système se réécrit alors sous la forme matricielle suivante:
\[
   U_{n} = AU_{n - 1}   
\]
Par récurrence on trouve alors que \(U_{n} = A^nU_1\), donc en particulier sachant \(U_1\), il nous suffit alors de calculer \(A^k\) comme précédemment ainsi que la matrice de passage et son inverse pour réussir à trouver le terme général de \(u_n, v_n\) et \(w_n\).\<

Plus subtilement, cette méthode s'applique aussi aux suites récurrentes d'ordre multiple, considérons par exemple la suite \(u_n\) de premiers termes \(u_1 = 1\) et \(u_2 = 2\) :
\[
   u_{n} = u_{n - 1} + u_{n - 2}   
\]
En effet si on pose \(U_n = \begin{pmatrix}
   u_{n-2} \\ u_{n-1} \\ u_n
\end{pmatrix}\)
alors on a l'expression matricielle:
\[
   U_{n} = \begin{pmatrix}0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 1 & 1\end{pmatrix}U_{n - 1}  
\]
Alors si la matrice est diagonalisable\footnote[1]{La matrice associée à la suite de Fibonacci n'est pas réductible dans \(\R\) donc on ne peut pas trouver une expression de son terme général.}, on peut alors trouver une expression de \(U_n\) en fonction de \(U_0\) et alors une expression de \(u_n\) simplement en fonction de \(n\).
\subsection*{\subsecstyle{Systèmes différentiels {:}}}
On considère trois fonctions réelles \(f, g, h\) de classes \(\mathcal{C}^1\) et on cherche à résoudre le système différentiel suivant:
\[
   \begin{cases}
      f'(x) = 2f(x) + g(x) + h(x)\\
      g'(x) = f(x) + 2g(x) + h(x)\\
      h'(x) = 3h(x)\\
   \end{cases}  
\]
On pose alors \(F(x) = \begin{pmatrix}
   f(x) \\ g(x) \\ h(x)
\end{pmatrix}\) et le système se réécrit alors sous la forme matricielle suivante:
\[
   F'(x) = AF(x) = PTP^{-1}F(x)
\]
On pose alors \(G(x) = P^{-1}F(x) = \begin{pmatrix} g_1(x) \\ g_2(x) \\ g_3(x) \end{pmatrix}\), alors on se ramène à l'équation matricielle suivante:
\[
   G'(x) = TG(x)    
\]
Alors on s'est ramené à un système triangulaire que l'on sait résoudre en partant du bas.\<

On peut donc résoudre pour \(G(x)\), et alors \(F(x)\) est égal à \(PG(x)\) (en utilisant la définition de \(G(x)\)) et on a donc trouvé les fonctions qui satisfont le système. Si on a des conditions initiales, on peut alors résoudre pour trouve l'unique triplet qui le satisfait.
